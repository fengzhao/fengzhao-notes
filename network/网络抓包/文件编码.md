# **文件类型和拓展名**

数据/文件以二进制的形式存储在计算机系统内，不同类型的数据有着其独有的特征用于被正确的应用程序识别，这些特征有文件名的扩展名、文件头标识等。

一般文件名可能有两部分组成如 新建文本文档.txt，其中.txt 这个文件名的后缀用于表示文件格式，被称为文件扩展名。

常见的扩展名有表示视频文件的.mp4、.avi，表示音频文件的.mp3、.wav、表示图像的.png、.jpg等。

其实在Linux中，不需要后缀名的，目前有些Linux下的文件有后缀名其实也只是为了符合Windows的用户以后缀名辨别文件类型的习惯而已。

一看文件名叫做temp.tar.gz，就知道这是个压缩包，但是这并不代表一定是对的，实际上我即使去掉了tar.gz，Linux照样可以识别出这是个压缩包文件类型。



在windows中，文件名一般都有拓展名，比如我们把 `docx` 拓展名的`word`文件，改成 exe 拓展名，在windows电脑中可能就无法打开了。

在windows7/windows10中，有个设置，叫为特定的文件或协议设置默认的应用，就是这个意思。

文件的扩展名是用来识别文件类型的。通过给他指定扩展名，我们可以告诉自己，也告诉操作系统我们想用什么方式打开这个文件。

比如操作系统会把.jpg的文件默认用图片显示软件打开，.zip 文件会默认用解压软件打开等等。



Linux中的file命令之所以可以判断二进制文件的类型，就是依靠这个文件头标识。

你即使更改了后缀名也不会影响这个文件头标识，所以依靠后缀名来判断常规文件类型是不靠谱的。



**文件头标志**

图像查看器不能正常显示文本文件中的内容，记事本打开图像文件也会显示乱码，这是因为二者的编码方式不同。

**每种类型的文件都有其特有文件编码方式，如在文件的头、尾或其他特定位置设置一定字节的数据用于标识文件类型。**

通常这些字节会设置在文件头的特定位置，被称为文件文件头标志（file signatures）或魔数（magic numbers, Magic Bytes）。

**查看文件头的工具**

在windows中可以用 UtraEdit 这个软件来打开任意文件，查看其文件头。

我们可以也利用 vscode 插件**hexdump for VSCode**以十六进制的形式查看二进制文件。

多打开几个文件试试，你会发现同一种类型的文件，他们的头信息是完全相同的。接下来，我们就可以根据头信息来判断文件类型了。

实测打开现代化的docx文件，对比其文件头，发现其本质上是zip文件。

如下图是两个png格式的图像文件用16进制打开后的内容，它们前面部分字节的内容是相同的，这些字节主要用于表示文件格式，后面的字节用于表示每个图像的元数据及图像信息。其中前8个字节的内容都是8950 4e47 0d0a 1a0a，这是表示png格式的文件头，其中50 4e47是PNG的ASCII码。

其实所有的文件都是以二进制存储的，都是01001010101...本质上没有什么差别。

在每一个文件（包括图片，视频或其他的各种非ASCII文件）的开头都有一片区域来显示这个文件的实际用法，这就是文件头标志，如果你要查看一个文件头的16进制标识，你可以采用Ultra-Edit这个工具尝试一下，常见的文件头标识如下：

LNK 4C 00 00 00 01 14 02 00 Windows shortcut file（这是Windows的快捷方式）

jar 5F 27 A8 89 JAR Archive File （这是jar包）

jpg; jpe; jpeg FF D8 FF E0 00 JPG Graphic File（这是JPG）

各种类型的文件头标志

https://www.cnblogs.com/gwind/p/8215771.html

https://blog.51cto.com/u_2982693/3354695

https://docs.fileformat.com/zh/page-description-language/svg/



**文件上传判断文件头**

对于有文件上传等业务系统，为了避免用户上传非法文件，上传可执行的webshell文件造成漏洞。

利用文件头判断文件类型：

上传文件时经常需要做文件类型判断，例如图片、文档等，普通做法是直接判断文件后缀名，而文艺青年为了防止各种攻击同时也会加上使用文件头信息判断文件类型。

常见的判断思路：

- 前端判断 
- 后端判断（文件）



对上传路径和上传文件重命名，即使传了可执行文件，攻击者也无法知晓访问路径，无法请求URL并执行。



## 文本文件和二进制文件

在学习C语言`fopen()`函数后，知道它的第二个参数是读取方式字符串。如果字符串中出现'b'，则表明是以二进制(binary)方式读取，否则是以文本方式读取。

计算机的存储在物理上是二进制的，也就是在物理存储方面没有区别都是 01 码。**文本文件与二进制文件的区别并不是物理上的，而是逻辑上的。**

这两者只是在编码层次上有差异。

简单来说，文本文件是基于字符编码的文件，常见的编码有ASCII编码，UNICODE编码等等。

二进制文件是基于值编码的文件，你可以根据具体应用，指定某个值是什么意思（这样一个过程，可以看作是自定义编码)



文本文件基本上是定长编码的(也有非定长的编码如UTF-8)。而二进制文件可看成是变长编码的，因为是值编码嘛，多少个比特代表一个值，完全由你决定。

> BMP文件格式，又称为Bitmap（位图）或是DIB(Device-Independent Device，设备无关位图)，是Windows系统中广泛使用的图像文件格式。
>
> 1、bmp文件头(bmp file header)：提供文件的格式、大小等信息
>
> 2、位图信息头(bitmap information)：提供图像数据的尺寸、位平面数、压缩方式、颜色索引等信息
>
> 3、调色板(color palette)：可选，如使用索引来表示图像，调色板就是索引与其对应的颜色的映射表
>
> 4、位图数据(bitmap data)：就是图像数据



文本工具打开一个文件的过程是怎样的呢？拿记事本来说，它首先读取文件物理上所对应的**二进制比特流，然后按照你所选择的解码方式来解释这个流**，然后将解释结果显示出来。一般来说，你选取的解码方式会是ASCII码形式（ASCII码的一个字符是８个比特），接下来，它8个比特8个比特地来解释这个文件流。

例如对于这么一个文件流"01000000_01000001_01000010_01000011"(下划线''_''，为了增强可读性手动添加的)，第一个8比特''01000000''按ASCII码来解码的话，所对应的字符是字符''A''，同理其它3个8比特可分别解码为''BCD''，即这个文件流可解释成“ABCD”，然后记事本就将这个“ABCD”显示在屏幕上。

事实上，世界上任何东西要与其他东西通信会话，都存在一个既定的协议，既定的编码。人与人之间通过文字联络，汉字“妈”代表生你的那个人，这就是一种既定的编码。但注意到这样一种情况，汉字“妈”在日本文字里有可能是你生下的那个人，所以当一个中国人Ａ与日本Ｂ之间用“妈”这个字进行交流，出现误解就很正常的。用记事本打开二进制文件与上面的情况类似。记事本无论打开什么文件都按既定的字符编码工作（如ASCII码），所以当他打开二进制文件时，出现乱码也是很必然的一件事情了，解码和译码不对应嘛。例如文件流''00000000_00000000_00000000_00000001''可能在二进制文件中对应的是一个四字节的整数int 1，在记事本里解释就变成了"NULL_NULL_NULL_SOH"这四个控制符。



文本文件格式存储时是**将值作为字符然后存入其字符编码的二进制**，文本文件用‘字符’作为单位来表示和存储数据，比如对于 1 这个值，文本文件会将其看做字符'1'然后保存其 ASCII 编码值（这里假定是 ASCII 编码），这样在物理上就是 `0x31` 这个二进制值。



而若是二进制保存 1，则直接保存其二进制值，比如如果程序中是处理 1 为整数则保存的二进制值就是 0x00000001 (4 字节）。 　　

文本文件的存储与其读取基本上是个逆过程。而二进制文件的存取显然与文本文件的存取差不多，只是编/解码方式不同而已，也不再叙述。

**假如文件存储的编码与读取的编码不同，那么就无法呈现文章原来的信息，例如用记事本打开文本文件会乱码，用音乐播放器无法打开视频文件。**



一般认为，文本文件编码基于字符定长，译码容易些；

二进制文件编码是变长的，所以它灵活，存储利用率要高些，译码难一些（不同的二进制文件格式，有不同的译码方式）。

于空间利用率，想想看，二进制文件甚至可以用一个比特来代表一个意思(位操作)，而文本文件任何一个意思至少是一个字符．



从编程的角度来说，Ｃ中文本或二进制读写都是缓冲区与文件中二进制流的交互，只是文本读写时有回车换行的转换。所以当写缓冲区中无换行符''\n''(0AH)，文本写与二进制写的结果是一样的，同理，当文件中不存在''\r\n''(0DH0AH)时，文本读与二进制读的结果一样。



文本文件与二进制文件就是编码方式不一样而已，而这个是用户行为，**把一个数据以什么样的编码（字符还是值本身）存入文件是由用户主动选择的，也就是写入的接口选择，如果以二进制接口方式写入文件那么就是一个二进制文件，如果以字符方式写入文件就是一个文本文件了。**既然有写入时候的编码也就会有读出的编码，只有两个编码对应才能读出正确的结果，如用记事本打开一个二进制文件会呈现乱码的，这里稍微提一下后缀名，后缀名并不能确定其是否就是文本文件，二进制文件也可以是 txt 后缀名，后缀名只是用来关联打开程序，给用户做备注用的，与文件的具体编码没有关系。 



# 文本文件编码

对于文本文件, 可以简单建立一个文本文件 “foo.txt”, 里面输入两个简单的字符， 比如 “hi”， 保存。 然后再查看文件的大小属性。

大小只有 2 字节, 也即 "hi" 两个字符的大小, 这意味着没有保存额外的所用编码的信息.



**文本文件仅仅是内容的字节序列, 没有其它额外的信息。**



保持内容不变, 简单地"另存为"一下, 在编码一栏选择 “UTF-8”, 再次查看属性将会发现大小变成了 5 :

再次查看十六进制形式时, 就会发现除了原来的 `68 69` 外, 还多出了 UTF-8 的 BOM: `ef bb bf`

以上三个与内容无关的字节使得大小变成了 5。这个信息是与所用编码有关的， 不过它仅能确定与 Unicode 相关的编码。

> 严格地说, BOM 的目的是用于确定字节序的。
>
> 另一方面，对于 UTF-8 而言， 现在通常不建议使用 BOM。因为 UTF-8 的字节序是固定的，所以很多的 UTF-8 编码的文本文件其实是没有 BOM 的。
>
> 
>
> 对 UTF-16 来说， BOM 是必须的， 因为它是存在字节序的， 弄反了字节序一个编码就会变成另一个编码了， 那就彻底乱套了。
>
> 不过一般很少用 UTF-16 编码来保存文件的, 更多是在内存中使用它作为一种统一的编码。
>
> 在 Windows 系统中, 统一用 UTF-16 编码, 直到按下保存时才会转换成 gbk 的字节数组写入硬盘. 这也是你为何可以在不同编码间的文档中能互相拷贝内容的原因。



**文本文件通常没有一个特殊的头部信息来供确定所用的编码。**



## 变相引入

文本文件中没有编码信息，导致了各种混乱。那么，最关键的就是要指定好所用的编码信息



什么是变相引入呢? 其实本质与前面提到的一些"文件头"信息是类似的。

我们来看看 xml 文件的例子, 你通常能在最开始看到这样的一行

```xml
<?xml version="1.0" encoding="UTF-8"?>
```

 encoding 指明的就是所用编码的信息。可是， 为了得到这一编码信息，我得先读取这一文件；可要正确读取文件，我又要先知道编码信息！





**HTML/JSP**

HTML 文件也会在 meta 标签中声明文件的编码方式。

智能一点的文本编辑器还会根据这一信息来作为保存时的编码。比如在 Eclipse 中, 如果设定了用 ISO-8859-1 编码，又同时录入了中文，还会出现保存时的警告。

像记事本这样傻乎乎的编辑器就没有这么贴心了。这时，保持宣称编码与实际保存用的编码一致就是源文件作者的责任了，否则可能不但没有帮助还会误导编辑器。

文档内的编码声明跟这个有点类似， 注意这个"内"字, 编码信息是文档内容本身的一部分。你想要正确的解析文档， 你首先要知道编码；而要知道编码, 你又要先解析文档…

所以这构成了一个死局, 关于怎么破解这个死局, 其实在之前的 引入编码信息的一些实践–乱码探源(3) 中有过介绍, 如果你感兴趣, 可以去看看, 其中还有一段利用正则表达式去获取编码信息的小程序示例。

简单讲, 就是先用所谓的"嗅探"加"猜测"的方式先做些尝试， 以尝试获取编码信息, 然后再用所获取的编码重新解析整个文档。

为提高嗅探的效率, html 规范建议文档内的 meta 编码信息应尽量放在前 1024 个字节内.。简单讲, 就是你最好就把它声明在 head 标签的第一行, 在 doctype 声明前面也不要写太多的注释, 否则, 嗅探算法扫描了前 1024 个字节仍然没有找到 meta charset 声明的特征字节码时, 就可能放弃, 认为你的文档内没有包含编码声明。



## 外部指定





# 如何确定一个网页的编码?

现在开始讲网页的编码，一个网页，简单讲就是一个 html 的 **文本文件**。那么如何去确定这样一个文本文件的编码呢?



概括地讲, 确定网页的编码有以下几种方式:

- 缺省
- 文档内的编码声明  
- 响应头中的 content-type 字段中的编码信息
- BOM

还有一种废弃的方式是在链接（link）中添加 charset 属性，比如像这样一个 a 标签

```html
<a href="/mysite/mydoc.html" charset="iso-8859-1">
```


如果没有使用其它几种方式指定编码， 就会用这里这个值。不过这种方式已经 不建议使用（deprecated）， 这里稍微提一提它。



**文档内声明**

文档内声明，其实就是 HTML 文件，自己在其文件最前面的几个字节中声明自己的编码方式。

```html
<!-- html4 写法 --> 
<meta http-equiv="Content-Type" content="text/html; charset=gbk"> 

<!-- html5 的标准写法 --> 
<meta charset="utf-8">   
```



**响应头中的 content-type 字段中的编码信息**

响应头的 charset 本质上是 HTTP Server 添加的描述性头部，告诉浏览器这个文本文件的编码方式。

```http
Content-Type: text/html; charset=gbk
```



**BOM**

具体以一个 UTF-8 的 BOM 来说吧，就是响应流中的开头的前三个字节， 它们是 `EF BB BF`, 这就是 UTF-8 的 BOM。 

如果有 BOM，就能直接知道对应的编码，无需 header 中的指示， 也无需用嗅探方式尝试获取流中包含的 `meta charset` 或 `meta http-equiv="Content-Type" `信息。



**缺省**

缺省方式就是以上几种方式都失效时的兜底方案，没有 BOM，没有 header，没有 meta。

浏览器只好"蒙"一个编码，当然也不是瞎蒙，通常跟系统语言地区设置有关，具体情况后面再说。



使情况变得复杂的是，几种方式有可能同时存在，比如文档内存在编码声明，同时响应头中也有编码信息。还有可能是冲突的：

**比如文档内的 meta 声明说编码是 GBK，响应头中的 content-type 却说编码是 UTF-8，那么浏览器此时该如何抉择呢?** 

这就涉及到置信度或者说优先级的问题了。



### 多方式并存且存在冲突时的优先级



 如果多方式并存, 且给出的编码信息不一致, 通常按这样的优先级来取舍:

1. BOM
2. 响应头编码
3. 文档内编码声明

也就是 BOM 的优先级是最高的，其次是响应头的，文档内编码声明的优先级最低。

而所谓通常，指新近的浏览器会按这样的方式处理，而较老版本的浏览器则可能有所偏差。

> 事实上, 协议本身在不断完善, 而各种浏览器实现众多, 版本也不断推陈出新, 加上实现人员对协议理解可能存在偏差等等原因, 这个问题想给个确切结论是比较困难的。





**文档内编码声明**

有的时候， 不是说你声明了某个编码， 你的文档就会自然而然地就用这个编码来保存。**这个一致性是要由源码的作者来保证的。**

如果两者不一致， 那么结果将会是乱码。比如页面 meta 宣称是 gbk 编码, 实际保存所用的却是 utf-8。当在浏览器中打开时, 将发生乱码。

这种文件在 IDE 中, 比如在 Eclipse 中打开时也是乱码的。因为较为智能的 IDE html 编辑器在打开这个文件时会参考里面 meta 声明的值来解码, 反而被误导了。

> 智能的 IDE html 编辑器在保存时也会参考 meta 声明的值来选择保存时的编码。这种不一致的情况要在外部的简易编辑器中去构建, 比如 notepad++ 编辑器
>

在某些浏览器中, 发生乱码时你可以手动的调整编码以获取正确的显示。比如firefox





**响应头编码**

优先级是要高过文档内编码声明的。 这其实也很好理解，因为它属于外部信息，而文档内编码声明则属于内部信息。

如果服务端的响应已经直接告诉你这段响应流的编码, 你都不用去"嗅探"了, 浏览器自然很乐意直接采用它。

也可以这样理解，这个编码的头信息服务端是可以不发送的，现在反而专门主动地告诉你，怕你不知道, 自然它的可信度也是很高的。浏览器优先采用它也不足为奇了，还省略了嗅探的步骤，提高了效率。

当在 tomcat8 缺省配置下运行时, 它不会为 Content-Type 中增加编码信息。我们需要主动去配置这个信息。具体而言, 可以去到工程的 web.xml 下, 在 `web-app` 根节点下增加一个 `mime-mapping` 配置



如果你直接修改 tomcat server 的相关配置文件, 就会成为服务器级别的, 将影响它下面部署的所有工程！

这时你要保证工程下所有的 html 文件都是使用 utf-8 来编码的, 否则由于它具有更高的优先级, 那些比如 gbk 编码的文档将会发生乱码



> 在开发实践中应该遵循"审慎"原则, 当你做某项调整时, 要清楚知道它的后果。你不能光顾着解决自己当下碰到的问题， 还要留心是否会引发更多的问题, 也即是人们常说的 “按下葫芦起了瓢”.很不幸的, 碰到乱码问题时, 我们经常是"病急乱投医", 而这点又往往是因为我们对编码问题没有一个整体认识所导致的
>





BOM声明



为什么 BOM 的优先级最高?

显然, BOM 其实类似于文档内编码声明, 不同之处则在于它是由文本编辑器控制写入的，而且是置于文档的最开头处。

假如说你选择了保存为 UTF-8 带 BOM 的编码，文本编辑器就会在内容之前添加"EF BB BF"三个字节，然后之后的内容也一定是用 UTF-8 来编码的。

不会发生在"文档内编码声明"中那种错误声明的情况.



这种一致性是由编辑器为我们保证的，除非你直接修改那些最终的二进制的值，否则这种宣称的 BOM 与实际所用的编码的一致性是能得到严格保证的。

换言之，就是它的置信度是最好的，所以它的优先级最高也就不难理解了。