# LVS 简介

LVS 是 Linux Virtual Server 的简写，意即 Linux 虚拟服务器，是一个虚拟的服务器集群系统，可以在 `UNIX/LINUX` 平台下实现负载均衡集群功能。

该项目在 1998 年 6 月由章文嵩博士组织成立，是中国国内最早出现的自由软件项目之一。那时他还在国防科技大学读博。LVS 最初是他在大学期间的玩具，随着后来使用的用户越来越多，LVS 也越来越完善，最终集成到了 Linux 的内核中。

不少开源牛人都为 LVS 开发过辅助工具和辅助组件，最出名的就是 `Alexandre` 为 LVS 编写的 Keepalived 。它最初专门用于监控 LVS ，后来加入了通过 VRRP 实现高可用的功能。

> 在 Linux 内核 2.2 时，IPVS 就已经以内核补丁的形式出现。
>
> 从 2.4.23 版本开始时，IPVS 软件就是合并到 Linux 内核的常用版本的内核补丁的集合。
>
> 从 2.4.24 以后，IPVS已经成为 Linux 官方标准内核的一部分。
>
> 
>
> 22年前国内的互联网还处在早期阶段，一台服务器加载一个网站都忙不过来，要多台服务器来解决，这就需要做负载均衡。
>
> 章文嵩发现，Linux内核里并没有这个功能，于是，他花了两个星期写了LVS软件放在网上，并给出使用文档。那是1998年5月，他还在国防科技大学读博。
>
> 一个星期内，他收到了诸多反馈。印象最深的是，一个澳洲人在社区网站用负载均衡调度网站政策来上网，这让他觉得很有成就感。很快，开发者们对这款软件提出更多需求，他与来自全球的不同开发者进行交流，乐此不彼地解决问题。
>
> 功不唐捐，LVS软件在负载均衡领域备受欢迎，他也成为Linux内核的重要开发者，但仍旧坚持开源。
>
> 参与开源实际上给他打开了一扇技术之窗。在与开发者互动的过程中，对章文嵩称写代码、设计系统等技术能力得到很大提升，也让其积累了诸多开发经验。在他看来，开发者实际上投入的精力，都有回报，“实际上真正把事情做好，所有的回报都会到来。”
>
> 
>
> 章文嵩的开源热情不减。多年来，他四处布道，鼓励技术人员参与开源项目。在阿里巴巴工作的近7年里，他历任淘宝网资深技术总监、阿里开源委员会主席、阿里云CTO等职务，其中负责开源了淘宝底层技术平台的上百个软件项目。
>
> 1998 年，章文嵩创办 LVS(Linux Virtual Server，Linux 虚拟服务器)开源软件拉开了国内开源产业的序幕，也成为了开源产业的领军人物。
>
> 在加入滴滴之前，章文嵩曾在阿里巴巴任职近七年，历任淘宝网资深技术总监、淘宝技术委员会主席、阿里副总裁、阿里开源委员会主席、阿里云 CTO 等。负责 ECS、RDS、OSS、CDN、SLB 等云产品，同时也在积极活跃开源事业。
>
> 章文嵩历任淘宝技术委员会主席、阿里集团开源委会会主席，引入开源文化，使得阿里因开源而受益，同时也提高了阿里的技术品牌和影响力。
>
> 2016 年，在章文嵩的加盟下，滴滴也开始加大开源力度。据了解，滴滴超五千多名研发人员所搭建维护的技术框架离不开开源软件的支持。通过大量自主开发软件与部分应用开源软件，滴滴构建了全世界规模最大的出行平台。
>
> 滴滴在 Github 全球组织排名已进入前 100 名。其源项目从单一的前端项目，覆盖技术领域扩大至人工智能、小程序、智慧交通、中间件、前端框架、研发工具等。
>
> 不过，章文嵩曾表示，"把软件开源出来只是万里长征的第一步，后续还要进行维护，持续投入才能把开源软件做得更好。"
>
> 
>
> 在滴滴任职期间，章文嵩辖下的，是技术中的基础问题，例如滴滴云、基础架构部、运维部、系统部、大数据架构部、IT&机器学习部、数据平台与数据治理部等。
>
> 在滴滴期间章文嵩也一直在推广开源理念，在章文嵩的带领下，滴滴受益于开源快速发展的同时降低了成本。
>
> 滴滴从2017年开始相继开源了23个项目开源，累计获得Star数36000多个，其中包括移动开发、中间件、系动软件、前端、研发、测试、智慧交通、人工智能等领域。
>
> 2021年初，章文嵩已经决定从滴滴离职，结合七八月份的滴滴上市和整治，现在回过头看，章文嵩还是有先见之明。



IPVS 是作为一个**Linux内核模块** 实现的，它与 **Netfilter** 框架紧密集成。

- **内核目录**：在 Linux 内核的源代码中，IPVS 的代码通常位于 `net/netfilter/ipvs` 子目录下。这表明它被视为 Netfilter 框架的一个扩展，用于处理网络数据包。
- **用户空间工具**：尽管 IPVS 本身在内核中运行，但它需要通过**用户空间工具**来配置。这个工具就是 **`ipvsadm`**。你可以通过 `ipvsadm` 命令来添加、修改或删除 IPVS 的虚拟服务器、真实服务器以及负载均衡调度算法。



LVS的全称是Linux virtual server，即Linux虚拟服务器。

之所以是虚拟服务器，是因为 LVS 自身是个负载均衡器(director)，不直接处理请求，而是将请求转发至位于它后端真正的服务器 `realserver` 上。





ipvs是集成在内核中的框架，可以通过用户空间的程序`ipvsadm`工具来管理，该工具可以定义一些规则来管理内核中的ipvs。就像iptables和netfilter的关系一样。



# LVS详解

`LVS`的全称是 `Linux virtual serve`，即Linux虚拟服务器。

==之所以是虚拟服务器，是因为 `LVS` 自身是个负载均衡器(director)，不直接处理请求，而是将请求转发至位于它后端真正的服务器 `realserver` 上。==



`LVS` 是四层(传输层`tcp/udp`)、七层(应用层：比如`http`)的负载均衡工具。只不过大众一般都使用它的四层负载均衡功能 `ipvs`。

> 在 `LVS` 的早期发展中，社区曾有过名为 `KTCPVS` 的项目，试图在内核中实现七层负载均衡。但这个项目并不成熟，也未被主流内核采纳，更多地是作为一个技术探索。
>
> 在现代的架构中，为了实现七层负载均衡，我们通常会使用专门的反向代理软件，比如 **Nginx、HAProxy 或 Envoy**。





`LVS` 的负载均衡调度技术是在 `Linux Kernel`中实现的，我们使用该软件配置 `LVS` 时候，不能直接配置内核中的 ipvs，而需要使用 `ipvs` 的管理工具 `ipvsadm` 进行管理。

后文我们会讲通过 `keepalive` 软件直接管理 `ipvs`，并不是通过 `ipvsadm` 来管理`ipvs`





`LVS` 主要由2部分程序组成：包括 `ipvs` 和 `ipvsadm`

1. `ipvs(ip virtual server)`：它是一段代码工作在内核空间，叫`ipvs`，是真正生效实现调度的代码。
2. `ipvsadm`：另外一段是工作在用户空间，叫`ipvsadm`，负责为`ipvs`内核框架编写规则，定义谁是集群服务，而谁是后端真实的服务器(Real Server)



使用 `LVS` 可以达到的技术目标是：通过 `LVS` 达到的负载均衡技术和 Linux 操作系统实现一个高性能高可用的 Linux 服务器集群，它具有良好的可靠性、可扩展性和可操作性。从而以低廉的成本实现最优的性能。

> 目前有三种IP负载均衡技术（VS/NAT,VS/TUN,VS/DR）

**`Virtual Server via Network Address Translation`（`VS/NAT`）**

通过NAT技术，调度器重写请求报文的目标地址，根据预设的调度算法，将请求分派给后端的真实服务器；真实服务器的响应报文通过调度器时，报文的源地址被重写，再返回给客户，完成整个负载调度过程。



`Virtual Server via IP Tunneling`（`VS/TUN`）
采用NAT技术时，由于请求和响应报文都必须经过调度器地址重写，当客户请求越来越多时，调度器的处理能力将成为瓶颈。为了解决这个问题，调度器把请求报 文通过IP隧道转发至真实服务器，而真实服务器将响应直接返回给客户，所以调度器只处理请求报文。由于一般网络服务应答比请求报文大许多，采用 VS/TUN技术后，集群系统的最大吞吐量可以提高10倍。

**`Virtual Server via Direct Routing`（`VS/DR`）**

`VS/DR`通过改写请求报文的MAC地址，将请求发送到真实服务器，而真实服务器将响应直接返回给客户。同`VS/TUN`技术一样，`VS/DR`技术可极大地 提高集群系统的伸缩性。

这种方法没有IP隧道的开销，对集群中的真实服务器也没有必须支持IP隧道协议的要求，但是要求调度器与真实服务器都有一块网卡连 在同一物理网段上。







`LVS`的IP负载均衡技术是通过`IPVS`模块来实现的，`IPVS`是`LVS`集群系统的核心软件，它的主要作用是：

1、`IPVS`安装在`Director Server`上，同时在`Director Server`上虚拟出一个IP地址，用户必须通过这个虚拟的IP地址访问服务。这个虚拟IP一般称为`LVS`的VIP，即Virtual IP。

2、访问的请求首先经过VIP到达负载调度器，然后由负载调度器从`Real Server`列表中选取一个服务节点响应用户的请求。 当用户的请求到达负载调度器后，调度器如何将请求发送到提供服务的`Real Server`节点





### LVS/NAT



(1). 当用户请求到达Director Server，此时请求的数据报文会先到内核空间的PREROUTING链。 此时报文的源IP为CIP，目标IP为VIP
(2). PREROUTING检查发现数据包的目标IP是本机，将数据包送至INPUT链
(3). IPVS比对数据包请求的服务是否为集群服务，若是，修改数据包的目标IP地址为后端服务器IP，然后将数据包发至POSTROUTING链。 此时报文的源IP为CIP，目标IP为RIP
(4). POSTROUTING链通过选路，将数据包发送给Real Server
(5). Real Server比对发现目标为自己的IP，开始构建响应报文发回给Director Server。 此时报文的源IP为RIP，目标IP为CIP
(6). Director Server在响应客户端前，此时会将源IP地址修改为自己的VIP地址，然后响应给客户端。 此时报文的源IP为VIP，目标IP为CIP



> LVS/NAT模型的特性

- RS应该使用私有地址，RS的网关必须指向DIP
- DIP和RIP必须在同一个网段内
- 请求和响应报文都需要经过Director Server，高负载场景中，Director Server易成为性能瓶颈
- 支持端口映射
- RS可以使用任意操作系统
- 缺陷：对Director Server压力会比较大，请求和响应都需经过`Director Server`

NAT（Network Address Translation 网络地址转换）是一种外网和内外地址映射的技术，内网可以是私有网址，外网可以使用NAT方法修改数据报头，让外网与内网能够互相通信。

在NAT模式下，网络数据报的进出都要经过`LVS`的处理。

`LVS`需作为RS（真实服务器）的网关。当包到达`LVS`时，`LVS`做目标地址转换（`DNAT`），将目标IP改为RS的IP。RS接收到包以后，仿佛是客户端直接发给它的一样。

RS处理完返回响应时，源IP是RS IP，目标IP是客户端的IP。这时RS的包通过网（`LVS`）中转，`LVS`会做源地址转换（`SNAT`），将包的源地址改为VIP，这样，这个包对客户端看起来就仿佛是`LVS`直接返回给它的。客户端无法感知到后端RS的存在。



缺点：在整个过程中，所有输入输出的流量都要经过LVS 调度服务器。显然，LVS 调度服务器的网络I/O压力将会非常大，因此很容易成为瓶颈，特别是对于请求流量很小，而响应流量很大的Web类应用来说尤为如此。

优点：NAT模式的优点在于配置及管理简单，由于了使用NAT技术，LVS 调度器及应用服务器可以在不同网段中，网络架构更灵活，应用服务器只需要进行简单的网络设定即可加入集群。



## LVS/DR

DR模式是通过改写请求报文的目标MAC地址，将请求发给真实服务器，而真实服务器响应后的处理结果直接返回给客户端用户。

**要求调度器DS与真实服务器RS都有一块网卡连接到同一物理网段上，必须在同一个局域网环境**。DR模式是互联网使用比较多的一种模式。



（1）RIP和DIP必须在同一个IP网络，且应该使用私网地址；RS的网关要指向DIP；
（2）请求报文和响应报文都必须经由Director转发；Director易于成为系统瓶颈；
（3）支持端口映射，可修改请求报文的目标PORT；
（4）vs必须是Linux系统，rs可以是任意系统；



1. 当用户向负载均衡调度器（Director Server）发起请求，调度器将请求发往至内核空间
2. `PREROUTING`链首先会接收到用户请求，判断目标IP确定是本机IP，将数据包发往INPUT链
3. `IPVS`是工作在INPUT链上的，当用户请求到达INPUT时，`IPVS`会将用户请求和自己已定义好的集群服务进行比对，如果用户请求的就是定义的集群服务，那么此时`IPVS`会强行修改数据包里的目标IP地址及端口，并将新的数据包发往`POSTROUTING`链
4. `POSTROUTING`链接收数据包后发现目标IP地址刚好是自己的后端服务器，那么此时通过选路，将数据包最终发送给后端的服务器



DR（Direct Routing 直接路由模式）此模式时`LVS`调度器只接收客户发来的请求并将请求转发给后端服务器，后端服务器处理请求后直接把内容直接响应给客户，而不用再次经过`LVS`调度器。

`LVS`只需要将网络帧的MAC地址修改为某一台后端服务器RS的MAC，该包就会被转发到相应的RS处理，注意此时的源IP和目标IP都没变。

RS收到`LVS`转发来的包时，链路层发现MAC是自己的，到上面的网络层，发现IP也是自己的，于是这个包被合法地接受，RS感知不到前面有`LVS`的存在。而当RS返回响应时，只要直接向源IP（即用户的IP）返回即可，不再经过`LVS`。







https://www.cnblogs.com/rexcheny/p/10778567.html

https://www.cnblogs.com/liugp/p/17294687.html

https://www.keepalived.org/pdf/sery-lvs-cluster.pdf