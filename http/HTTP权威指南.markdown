# HTTP简介

HTTP概述

https://www.cnblogs.com/huansky/p/13124807.html



HTTP（HyperText Transfer Protocol）即超文本传输协议，是一种详细规定了浏览器和万维网服务器之间互相通信的规则，它是万维网交换信息的基础。

它允许将HTML（超文本标记语言）文档从Web服务器传送到Web浏览器。



## HTTP工作原理：

HTTP协议工作于客户端-服务端架构上。浏览器作为 HTTP 客户端通过 URL 向 HTTP 服务端即 WEB 服务器发送所有请求。

Web服务器有：Apache服务器，IIS服务器（Internet Information Services），Nginx 等。

Web服务器根据接收到的请求后，向客户端发送响应信息。

HTTP默认端口号为80，但是你也可以改为8080或者其他端口。

HTTPS 的默认端口是443 



# HTTP头部





## 通用头部



### 通用信息头部





### 通用缓存头部



## 请求头部







## 响应头部



## 实体头部





## 扩展头部



### X-Forwarded-For

X-Forwarded-For 是一个 HTTP 扩展头部。

HTTP/1.1（RFC 2616）协议并没有对它的定义，它最开始是由 Squid 这个缓存代理软件引入，用来表示 HTTP 请求端真实 IP。

如今它已经成为事实上的标准，被各大 HTTP 代理、负载均衡等转发服务广泛使用，并被写入 [RFC 7239](http://tools.ietf.org/html/rfc7239)（Forwarded HTTP Extension）标准之中。



> X-Forwarded-For 请求头格式
>
> X-Forwarded-For: client, proxy1, proxy2

可以看到，XFF 的内容由「英文逗号 + 空格」隔开的多个部分组成，最开始的是离服务端最远的设备 IP，然后是每一级代理设备的 IP。

如果一个 HTTP 请求到达最后端的 HTTP 服务器之前。

经过了三个代理 Proxy1、Proxy2、Proxy3，IP 分别为 IP1、IP2、IP3，用户真实 IP 为 IP0，那么按照 XFF 标准，服务端最终会收到以下信息：



> X-Forwarded-For: IP0, IP1, IP2



Proxy3 直连服务器，它会给 XFF 追加 IP2，表示它是在帮 Proxy2 转发请求。

**Remote Address** 

HTTP 连接基于 TCP 连接，Remote Address 来自 TCP 连接，表示与服务端建立 TCP 连接的设备 IP 。

> 铁律：当多层代理或使用CDN时，如果代理服务器不把用户的真实IP传递下去，那么业务服务器将永远不可能获取到用户的真实IP。



**X-Real-IP**

这是一个自定义 HTTP 头部。

`X-Real-IP` 通常被 HTTP 代理用来表示与它产生 TCP 连接的设备 IP 。这个设备可能是其他代理，也可能是真正的请求端。

需要注意的是，`X-Real-IP` 目前并不属于任何标准，代理和 Web 应用之间可以约定用任何自定义头来传递这个信息。



列表中并没有 IP3，在服务端通过 Remote Address 字段获得 IP3。

我们知道 HTTP 连接基于 TCP 连接，HTTP 协议中没有 IP 的概念，Remote Address 来自 TCP 连接，表示与服务端建立 TCP 连接的设备 IP 。

Remote Address 无法伪造，因为建立 TCP 连接需要三次握手，如果伪造了源 IP，无法建立 TCP 连接，更不会有后面的 HTTP 请求。

不同语言获取 Remote Address 的方式不一样，例如 php 是 `$_SERVER["REMOTE_ADDR"]`，Node.js 是 `req.connection.remoteAddress`，但原理都一样 。

很多 web 应用，也要分析判断头部，来取真正的请求的客户端 IP 。



在 nginx 中，通常可以这样配置

```nginx

    location /proxy {                                                                                                               	 proxy_http_version 1.1;                                                                                                     	  proxy_set_header Connection "";
    	# 协议升级头部
    	proxy_set_header Upgrade $http_upgrade; 
    	 # 协议升级头部 
    	proxy_set_header Connection "upgrade";                                                                       					proxy_set_header Host $host;                                                                                           		 proxy_set_header X-Real-IP $remote_addr;                                                                               		 proxy_set_header REMOTE-HOST $remote_addr; 
    	 # 
    	 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;                                                           			
    	 proxy_pass http://localhost/index.html;  
```

https://imququ.com/post/x-forwarded-for-header-in-http.html



#### 动态NAT场景

比如家里电信宽带上网，电信给分配了公网ip，那么一个访问公网网站的请求经过的 ip 路径如下：

- 192.168.0.101(用户电脑ip)

- 192.168.0.1/116.1.2.3(路由器的局域网ip/路由器得到的电信公网ip)

- 119.147.19.234(业务的前端负载均衡服务器)
- 192.168.126.127(业务处理服务器)。

在这种情况下，对于服务器后端业务服务器，应该取到的客户端IP应该是 116.1.2.3 ，可以在代码中通过头部进行判断



#### 端口NAT场景

宽带提供商没有足够的公网ip，分配到用户的接入网的是个私网ip，比如长宽等小的isp。**现在基本上都是这种网络**

- 192.168.0.123(用户电脑ip)
- 192.168.0.1/10.0.1.2(路由器的局域网ip及路由器得到的运营商内网ip)
- 211.162.78.1（网络运营商长城宽带的公网ip）
- 119.147.19.234(业务的前端负载均衡服务器)
- 192.168.126.127(业务处理服务器)。

在这种情况下，对于服务器后端业务服务器，应该取到的客户端应该是 211.162.78.1，可以在代码中通过头部进行判断









示例  



```javascript
// 启动一个 nodejs 的应用，监听9009端口
var http = require('http');

http.createServer(function (req, res) {
    res.writeHead(200, {'Content-Type': 'text/plain'});
    res.write('remoteAddress: ' + req.connection.remoteAddress + '\n');
    res.write('x-forwarded-for: ' + req.headers['x-forwarded-for'] + '\n');
    res.write('x-real-ip: ' + req.headers['x-real-ip'] + '\n');
    res.end();
}).listen(9009, '0.0.0.0');
```





```golang
package main
 
import (
 	// Standard library packages
    "fmt"
    "log"
    "net"
    "net/http" 
)
 
// w表示response对象，返回给客户端的内容都在对象里处理
// r表示客户端请求对象，包含了请求头，请求参数等等
func indexHandler(w http.ResponseWriter, r *http.Request) {
    // 往w里写入内容，就会在浏览器里输出
    fmt.Fprintf(w, "Hello golang http!")
    // 
    fmt.Fprintf(w, "")
}


// RemoteIp 返回远程客户端的 IP，如 192.168.1.1
func RemoteIp(req *http.Request) string {
	remoteAddr := req.RemoteAddr
	if ip := req.Header.Get(XRealIP); ip != "" {
		remoteAddr = ip
	} else if ip = req.Header.Get(XForwardedFor); ip != "" {
		remoteAddr = ip
	} else {
		remoteAddr, _, _ = net.SplitHostPort(remoteAddr)
	}

	if remoteAddr == "::1" {
		remoteAddr = "127.0.0.1"
	}

	return remoteAddr
}


func main() {

    // 设置路由，如果访问/，则调用index方法
    http.HandleFunc("/", indexHandler)
    http.HandleFunc("/", indexHandler)
    
    RemoteIp()
 
    // 启动web服务，监听9090端口
    err := http.ListenAndServe(":9090", nil)
    if err != nil {
        log.Fatal("ListenAndServe: ", err)
    }
}
```











# HTTP报文



HTTP 报文是服务器和客户端之间交换数据的方式，有两种类型的消息︰

- 请求（requests）--由客户端发送用来触发一个服务器上的动作；
- 响应（responses）--来自服务器的应答。

请求



HTTP消息由采用 ASCII 编码的多行文本构成。在HTTP/1.1及早期版本中，这些消息通过连接公开地发送。

在HTTP/2中，为了优化和性能方面的改进，HTTP报文被分到多个HTTP帧中。



# HTTP连接管理



## TCP连接

TCP 负责在不可靠的传输信道之上提供可靠的抽象层，向应用层隐藏了大多数网络通信的复杂细节，比如丢包重发、按序发送、拥塞控制及避免、数据完整等等。

TCP 为 HTTP 提供了一条可靠的比特传输管道。从 TCP 连接一端填入的字节会从另一端以原有的顺序、正确地传送出来。

**TCP 会按序、无差错地承载 HTTP 数据。**

HTTP 标准并未规定 TCP 就是唯一的传输协议。如果你愿意，还可以通过 UDP（用户数据报协议）或者其他可用协议来发送 HTTP 消息。

但在现实当中，由于 TCP 提供了很多有用的功能，几乎所有 HTTP 流量都是通过 TCP 传送的。



HTTP 要传送一条报文时，会以流的形式将报文数据的内容通过一条打开的 TCP 连接按序传输。

TCP 收到数据流之后，会将数据流砍成被称作段的小数据块，并将段封装在 IP 分组中，通过因特网进行传输。

所有这些工作都是由 TCP/IP 软件来处理的，HTTP 程序员什么都看不到。





每个 TCP 段都是由 IP 分组承载，从一个 IP 地址发送到另一个 IP 地址的。每个 IP分组中都包括：

- 一个 IP 分组首部（通常为 20 字节）：IP 首部包含了源和目的 IP 地址、长度和其他一些标记
- 一个 TCP 段首部（通常为 20 字节）：TCP 段的首部包含了 TCP端口号、TCP 控制标记，以及用于数据排序和完整性检查的一些数字值
- 一个 TCP 数据块（0 个或多个字节）





在任意时刻计算机都可以有几条 TCP 连接处于打开状态。TCP 是通过端口号来保持所有这些连接持续不断地运行。

**TCP 连接是通过四元组来唯一标识的：< 源 IP 地址:源端口号——目的 IP 地址:目的端口号 >**

**唯一标识：两条不同的 TCP 连接不能拥有 4 个完全相同的值（但不同连接的部分组件可以拥有相同的值）**





### TCP 三次握手



所有 TCP 连接一开始都要经过三次握手，客户端与服务器在交换应用数据之前，必须就起始分组序列号，以及其他一些连接相关的细节达成一致。

出于安全考虑，序列号由两端随机生成。





- SYN 
  客户端选择一个随机序列号 x，并发送一个 SYN 分组，其中可能还包括其他 TCP标志和选项。

  

- SYN ACK 
  服务器给 x 加 1，并选择自己的一个随机序列号 y，追加自己的标志和选项，然后返回响应。

  

- ACK 
  客户端给 x 和 y 加 1 并发送握手期间的最后一个 ACK 分组。





![1602948494101](assets/1602948494101.png)





三次握手带来的延迟使得每创建一个新 TCP 连接都要付出很大代价。而这也决定了提高 TCP 应用性能的关键，在于想办法重用连接。





#### TCP三次握手时延

------

1、请求新的 TCP 连接时，客户端要向服务器发送一个小的 TCP 分组（通常是 40 ～60 个字节）。这个分组中设置了一个特殊的 SYN 标记，说明这是一个连接请求。

2、如果服务器接受了连接，就会对一些连接参数进行计算，并向客户端回送一个TCP 分组，这个分组中的 SYN 和 ACK 标记都被置位，说明连接请求已被接受。

3、最后，客户端向服务器回送一条确认信息，通知它连接已成功建立。现代的 TCP 栈都允许客户端在这个确认分组中发送数据。



由于 HTTP 是在 TCP 连接上传输数据的。所以 TCP 的三次握手的时延，也是很大开销。

### TCP流量控制

------

**什么是流量控制**

流量控制是一种预防发送端过多向接收端发送数据的机制。

一条 TCP 连接的每一侧主机都会为该连接设置了**接收缓存**。当该 TCP 连接收到正确、按序的字节后，它就将数据放入**接收缓存**。

相关应用进程会从该缓存中读取数据。事实上，接收方应用可能由于繁忙等原因，并没有马上读取。如果发送方的数据发的太快，很容易使得**接收缓存溢出**。





**为什么要流量控制**

如果缓存区满了发送方还在疯狂着发送数据，接收方只能把收到的数据包丢掉，大量的丢包会极大着浪费网络资源，因此，我们需要控制发送方的发送速率，让接收方与发送方处于一种动态平衡才好。



**流量控制的实现**

为实现流量控制，TCP 连接中的数据的发送方要向对方通告自己的 **接收窗口**（rwnd）变量，用于表示接收能力，其中包含能够保存数据的缓冲区空间大小信息。



由于 TCP 是全双工通信，所以 TCP 连接的两端都要 向对方提供这个变量。接收端将此窗口值放在 TCP 报文的首部中的窗口字段，传送给发送端。



第一次建立连接时，两端都会使用自身系统的默认设置来发送 rwnd 。

浏览网页通常主要是从服务器向客户端下载数据，因此客户端窗口更可能成为瓶颈。

如果是在上传图片或视频，即客户端向服务器传送大量数据时，服务器的接收窗口又可能成为制约因素。



由于 TCP 是全双工通信，所以 TCP 连接的两端都要 向对方提供这个变量。

对于 CDN 服务器，initcwnd 的设置尤其重要。CDN服务器充当您的内容和用户之间的代理，具有两个角色：

- 服务器-响应用户的请求。

- 客户端-向原始服务器发出请求（在MISS缓存上）

  

![浏览器，CDN和原始服务器的交互以及TCP性能设置](https://www.cdnplanet.com/static/img/tcp-performance-tuning_2.png)



现在，当CDN充当*服务器*时，CDN上的 initcwnd 设置是确定新连接的第一段大小的重要因素。但是，当CDN充当*客户端*角色时，如果原始服务器的initcwnd高于CDN服务器的initrwnd，则CDN的广告接收窗口将成为瓶颈。

由于大多数 CDN 都无法保持与原始站点的长连接，因此高速缓存MISS两次启动缓慢！

   

### TCP慢启动

------



在网络实际的传输数据过程中，会出现拥塞的现象，网络上充斥着非常多的数据包，但是却不能按时被传送，形成网络拥塞，其实就是和平时的堵车一个性质了。

TCP 设计中也考虑到这一点，使用了一些算法来检测网络拥塞现象，如果拥塞产生，变会调整发送策略，减少数据包的发送来缓解网络的压力。

拥塞控制主要有四个算法：

- 慢启动
- 拥塞避免
- 拥塞发生时，快速重传
- 快速恢复

慢启动实现：

慢启动为发送方的 TCP 增加了一个窗口变量：拥塞窗口，记为 cwnd，初始化之后慢慢增加这个 cwnd 的值来提升速度。同时也引入了 ssthresh 门限值，如果 cwnd 达到这个值会让 cwnd 的增长变得平滑，算法如下：

1. 连接建好的开始先初始化cwnd = 1，表明可以传一个MSS大小的数据
2. 每当收到一个ACK，cwnd++; 呈线性上升
3. 每当过了一个RTT，cwnd = cwnd*2; 呈指数让升
4. 当cwnd >= ssthresh时，就会进入“拥塞避免算法”

简单来说，每成功接收一个分组，发送端就有了发送另外两个分组的权限。

如果某个 HTTP 事务有大量数据要发送，是不能一次将所有分组都发送出去的。必须发送一个分组，等待确认；然后可以发送两个分组，每个分组都必须被确认，这样就可以发送四个分组了，

以此类推。这种方式被称为“打开拥塞窗口”。

由于存在这种拥塞控制特性，所以新连接的传输速度会比已经交换过一定量数据的、“已调谐”的连接慢一些.



### 串行事务处理时延

假设有一个包含了 3 个嵌入图片的 Web 页面。浏览器需要发起 4 个 HTTP 事务来显示此页面：
1 个用于顶层的 HTML 页面，3 个用于嵌入的图片

如果每个事务都需要（串行地建立）一条新的连接，那么连接时延和慢启动时延就会叠加起来：



### 并行连接（多个TCP连接）

并行连接，意思就是打开多个TCP连接，HTTP 允许客户端打开多条 TCP 连接，并行地执行多个 HTTP 事务。

包含嵌入对象的组合页面如果能（通过并行连接）克服单条连接的空载时间和带宽限制，加载速度也会有所提高。

如果单条连接没有充分利用客户端的因特网带宽，可以将未用带宽分配来装载其他对象。

**即使并行连接的速度可能会更快，但并不一定总是更快。**

客户端的网络带宽不足时，大部分的时间可能都是用来传送数据的。在这种情况下，一个连接到速度较快服务器上的HTTP 事务就会很容易地耗尽所有可用的 Modem 带宽。

如果并行加载多个对象，每个对象都会去竞争这有限的带宽，每个对象都会以较慢的速度按比例加载，这样带来的性能提升就很小，甚至没什么提升。



**并行连接的问题**

- 每个事务都会打开 / 关闭一条新的连接，会耗费时间和带宽。 
- 由于 TCP 慢启动特性的存在，每条新连接的性能都会有所降低。 
- 可打开的并行连接数量实际上是有限的。 （浏览器并发请求限制）



### HTTP Pipelining 

HTTP/1.1 存在一个问题，单个 TCP 连接在同一时刻只能处理一个请求。

意思是说：两个请求的生命周期不能重叠，任意两个 HTTP 请求从开始到结束的时间在同一个 TCP 连接里不能重叠。



 一个支持持久连接的客户端可以在一个TCP连接中发送多个请求（不需要等待任意请求的响应）。收到请求的服务器必须按照请求收到的顺序发送响应。







### 持久连接（TCP长连接）

https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Connection_management_in_HTTP_1.x

持久连接就是 TCP 连接的重用，一个 Web 页面上的大部分内嵌图片通常都来自同一个 Web 站点，而且相当一部分指向其他对象的超链通常都指向同一个站点。

> 初始化了对某服务器 HTTP 请求的应用程序很可能会在不久的将来对那台服务器发起更多的请求，这种特性称为站点局部性（site locality）

在`HTTP/1.0`中，一个 http 请求收到服务器响应后，默认会断开对应的 TCP 连接。

这样每次请求，都需要重新建立 TCP 连接，这样一直重复建立和断开的过程。简直就是灾难。



所以为了复用 TCP 连接。在 http/1.0 中规定了两个头部 Connection 和 keep-alive，可以设置头字段`Connection: keep-alive`。这是一个协商头部。

客户端可以通过包含 `Connection: Keep-Alive`请求头将一条 TCP 连接保持在打开状态。（客户端说，我想保持 TCP 长连接）

如果服务器愿意为了下一条 HTTP 请求，将 TCP 连接保持在打开状态，就在响应中包含相同的响应头部  `Connection: Keep-Alive` 。（服务器说，我支持 TCP 长连接）

这样 HTTP 请求完成后，就不会断开当前的 TCP 连接，后续的 HTTP 请求可以使用当前 TCP 连接进行通信。（这样，就维持了长连接）

在 chrome F12 中，点击一个 HTTP 事务，查看 timing 时，如果看到初始化连接和SSL开销消失了，说明使用的是同一个TCP连接。

如果响应头中没有 `Connection: Keep-Alive`，客户端就认为服务器不支持 keep-alive，会在发回响应报文之后主动关闭 TCP 连接。

> 
>
> 域名哈希技术
>
> 头条面试题一：
>
> 网页中的图片资源为什么分放在不同的域名下 ?  
>
> 比如知乎中的图片资源的URL通常是 https://pic1.zhimg.com/013926975_im.jpg ，与主站的域名不同。
>
> 一、
>
> 其实就是域名哈希技术（或者说域名分区）
>
> 由于浏览器针对同一个域名的并发请求是有限制的，Chrome是 6 个。即在Chrome 中访问 zhihu.com 时，本地与 zhihu.com 同时最多只能有 6 个 TCP 连接。
>
> 浏览器对并发请求的数目限制是针对域名的，即针对同一域名（包括二级域名）在同一时间支持的并发请求数量的限制。
>
> 如果请求数目超出限制，则会阻塞。因此，网站中对一些静态资源，使用不同的一级域名，可以提升浏览器并行请求的数目，加速界面资源的获取速度。
>
> 但是，每个新主机也会有一次额外的 DNS 查询，也有更多的开销。
>
> 在实践中，可以用一个服务器托管资源，然后 cname 解析多个域名到同一台服务器。由于浏览器的限制是对域名的限制，并没有对 IP 的限制，这样就可以突破限制。
>
> 但是这种域名分区并不是越多越好，很明显，如果一个客户端对服务器建立太多 TCP 连接，自然会过多消耗服务器资源。
>
> 
>
> 浏览针对同一个域名的并发请求限制到底是什么意思？







```shell

# Connection响应首部和Keep-Alive响应首部
# Keep-Alive 首部完全是可选的，但只有在提供 Connection: Keep-Alive 时才能使用它
# timeout  估计了服务器希望将连接保持在活跃状态的时间，这并不是一个承诺值。
# max      。这并不是一个承诺值。
Connection: Keep-Alive
Keep-Alive: max=5, timeout=120
# 这个例子说明服务器最多还会为另外 5 个事务保持连接的打开状态，或者将打开状态保持到连接空闲了 2 分钟之后
```



**Keep-Alive 连接的限制和规则**

- 在 HTTP/1.0 中，keep-alive 并不是默认使用的。 客户端必须发送一个 Connection: Keep-Alive 请求首部来激活 keep-alive 连接。
- 发送了 Connection: close 请求首部之后，客户端就无法在那条连接上发送更多的请求了。
- HTTP/1.1 的代理必须能够分别管理与客户端和服务器的持久连接——每个持久连接都只适用于一跳传输。
- 哑代理，Keep-Alive首部是针对单条TCP链路的，逐跳首部只与一条特定的连接有关，不能被转发。
- 所以现代的代理在转发报文时，要去掉 Connection 和 Keep-Alived 首部
- 



在`HTTP/1.1`中，将`Connection`写入了标准，默认值为`keep-alive`。除非强制设置为`Connection: close`，才会在请求后断开TCP连接。



**事务执行结束之后保存打开状态的连接，叫持久连接。（实际上就是TCP的长连接）**

**长连接可能的问题**：

> **不小心就会累积出大量的空闲连接，耗费本地以及远程客户端和服务器上的资源。**
>
> **存活功能的探测周期太长**，还有就是它只是探测TCP连接的存活，属于比较斯文的做法，遇到恶意的连接时，保活功能就不够使了。
>
> 如果条件再允许就可以以客户端机器为颗粒度，限制每个客户端的最大长连接数，这样可以完全避免某个蛋疼的客户端连累后端服务。



**短连接**

> **短连接**对于服务器来说管理较为简单，存在的连接都是有用的连接，不需要额外的控制手段。
>
> 但如果客户**请求频繁**，将在**TCP的建立和关闭操作上浪费时间和带宽**。
>
> 长连接和短连接的产生在于client和server采取的关闭策略，具体的应用场景采用具体的策略，没有十全十美的选择，只有合适的选择。



在 HTTP/2 中，由于





### 管道化连接

在持久连接的基础上，管道化连接更进一步，客户端可以将大量请求放入队列中排队。



副作用是很重要的问题。如果在发送出一些请求数据之后，收到返回结果之前，连接关闭了，客户端就无法百分之百地确定服务器端实际激活了多少事务。

有些事务，比如 GET 一个静态的 HTML 页面，可以反复执行多次，也不会有什么变化。

而其他一些事务，比如向一个在线书店 POST 一张订单，就不能重复执行，不然会有下多张订单的危险。

#### **幂等事务**

如果一个事务，不管是执行一次还是很多次，得到的结果都相同，这个事务就是幂等的。



客户端不应该以管道化方式传送非幂等请求，否则，传输连接的过早终止就会造成一些不确定的后果。要发送一条非幂等请求，就需要等待来自前一条请求的响应状态。

大多数浏览器都会在重载一个缓存的 POST 响应时提供一个对话框，询问用户是否希望再次发起事务处理。





### 正常关闭连接

TCP 连接是双向的。TCP 连接的每一端都有一个输入队列和一个输出队列，用于数据的读或写。

放入一端输出队列中的数据最终会出现在另一端的输入队列中。

#### 完全关闭和半关闭

应用程序可以关闭 TCP 输入和输出信道中的任意一个，或者将两者都关闭了。套接字调用 close() 会将 TCP 连接的输入和输出信道都关闭了，这被称作“完全关闭”

还可以用套接字调用 shutdown() 单独关闭输入或输出信道。这被称为“半关闭”

关闭连接的输出信道总是很安全的。连接另一端的对等实体会在从其缓冲区中读出所有数据之后收到一条通知，说明流结束了，这样它就知道你将连接关闭了。

关闭连接的输入信道比较危险，除非你知道另一端不打算再发送其他数据了。

如果另一端向你已关闭的输入信道发送数据，操作系统就会向另一端的机器回送一条TCP “连接被对端重置” 的报文

**HTTP 规范建议，当客户端或服务器突然要关闭一条连接时，应该“正常地关闭传输连接”，**



> **
>
> 
>
> 
>
> 
>
> 
>
> 
>
> 
>
> 
>
> 
>
> 















## HTTP时延

与建立 TCP 连接，以及传输请求和响应报文的时间相比，事务处理时间可能是很短的。

**除非服务端或客户端超载有性能问题，否则 HTTP 时延就是由 TCP 网络时延构成的。**



1、域名解析。

2、建立TCP连接，TCP三次握手，四次断开。

3、客户端通过TCP管道发送HTTP请求报文。

4、服务器收到报文后，进行处理，生成响应报文。

5、通过TCP管道向客户端发送响应报文。

6、客户端浏览器收到报文，并解析渲染页面。



### TCP三次握手时延

------

1、请求新的 TCP 连接时，客户端要向服务器发送一个小的 TCP 分组（通常是 40 ～60 个字节）。这个分组中设置了一个特殊的 SYN 标记，说明这是一个连接请求。

2、如果服务器接受了连接，就会对一些连接参数进行计算，并向客户端回送一个TCP 分组，这个分组中的 SYN 和 ACK 标记都被置位，说明连接请求已被接受。

3、最后，客户端向服务器回送一条确认信息，通知它连接已成功建立。现代的 TCP 栈都允许客户端在这个确认分组中发送数据。



### 延时确认

------

由于因特网自身无法确保可靠的分组传输（因特网路由器超负荷的话，可以随意丢弃分组，IP协议是不可靠的），所以 TCP 实现了自己的确认机制来确保数据的成功传输。

> 每个 TCP 段都有一个序列号和数据完整性校验和。每个段的接收者收到完好的段时，都会向发送者回送小的确认分组。
>
> 如果发送者没有在指定的窗口时间内收到确认信息，发送者就认为分组已被破坏或损毁，并重发数据。



由于确认报文很小，所以 TCP 允许在发往相同方向的输出数据分组中对其进行“捎带”。TCP 将返回的确认信息与输出的数据分组结合在一起，可以更有效地利用网络。

为了增加确认报文找到同向传输数据分组的可能性，很多 TCP 栈都实现了一种“延迟确认”算法。即返回的确认分组先不发出去，在主机的TCP缓冲区中等待。

延迟确认算法会在一个特定的窗口时间（通常是 100 ～ 200 毫秒）内将输出确认存放在缓冲区中，以寻找能够捎带它的输出数据分组。

如果在那个时间段内没有输出数据分组，就将确认信息放在单独的分组中传送。



但是，HTTP 具有双峰特征的请求 - 应答行为降低了捎带信息的可能。当希望有相反方向回传分组的时候，偏偏没有那么多。通常，延迟确认算法会引入相当大的时延。根据所使用操作系统的不同，可以调整或禁止延迟确认算法。

在对 TCP 栈的任何参数进行修改之前，一定要对自己在做什么有清醒的认识。TCP中引入这些算法的目的是防止设计欠佳的应用程序对因特网造成破坏。

对 TCP 配置进行的任意修改，都要绝对确保应用程序不会引发这些算法所要避免的问题。





### TCP慢启动

------



在网络实际的传输过程中，会出现拥塞的现象，网络上充斥着非常多的数据包，但是却不能按时被传送，形成网络拥塞，其实就是和平时的堵车一个性质了。

TCP设计中也考虑到这一点，使用了一些算法来检测网络拥塞现象，如果拥塞产生，变会调整发送策略，减少数据包的发送来缓解网络的压力。

拥塞控制主要有四个算法：

- 慢启动
- 拥塞避免
- 拥塞发生时，快速重传
- 快速恢复

慢启动为发送方的TCP增加了一个窗口：拥塞窗口，记为 cwnd，，初始化之后慢慢增加这个 cwnd 的值来提升速度。同时也引入了 ssthresh 门限值，如果 cwnd 达到这个值会让 cwnd 的增长变得平滑，算法如下：

1. 连接建好的开始先初始化cwnd = 1，表明可以传一个MSS大小的数据

2. 每当收到一个ACK，cwnd++; 呈线性上升

3. 每当过了一个RTT，cwnd = cwnd*2; 呈指数让升

4. 当cwnd >= ssthresh时，就会进入“拥塞避免算法”

简单来说，每成功接收一个分组，发送端就有了发送另外两个分组的权限。

如果某个 HTTP 事务有大量数据要发送，是不能一次将所有分组都发送出去的。必须发送一个分组，等待确认；然后可以发送两个分组，每个分组都必须被确认，这样就可以发送四个分组了，

以此类推。这种方式被称为“打开拥塞窗口”。

由于存在这种拥塞控制特性，所以新连接的传输速度会比已经交换过一定量数据的、“已调谐”的连接慢一些.





### Nagle算法

TCP 有一个数据流接口，应用程序可以通过它将任意尺寸的数据放入 TCP 栈中——即使一次只放一个字节也可以！



每个 TCP 段中都至少装载了 40 个字节的标记和首部，所以如果 TCP 发送了大量包含少量数据的分组，网络的性能就会严重下降。

> 发送大量单字节分组的行为称为“发送端傻窗口综合症”。这种行为效率很低、违反社会道德，而且可能会影响其他的因特网流量。





## HTTP连接的处理

### connection首部



HTTP 允许在客户端和最终的源端服务器之间存在一串 HTTP 中间实体（代理，缓存等）

可以从客户端开始，逐跳地将 HTTP 报文经过这些中间设备，转发到源端服务器上去（或者进行反向传输）。



HTTP 应用程序收到一条带有 Connection 首部的报文时，接收端会解析发送端请求的所有选项，并将其应用。然后会在将此报文转发给下一跳地址之前，删除Connection 首部以及 Connection 中列出的所有首部。









## 代理

代理就是帮助你（client）处理与服务器（server）连接请求的中间应用，我们称之为 proxy server。



### 正向代理

对于 client，想要连上 server，如果这个中间过程你知道代理服务器的存在，那这就是正向代理（即这个过程是 client 主动的过程）。

比如我们在 Chrome 上配置 SwitchOmega 工具来使用代理，设置一系列代理规则，可以先将请求发到代理服务器，由代理服务器再去发送请求。

比如我们在一些开发工具（比如 jetbrains idea）中，配置 proxy 来设置代理服务器。

比如我们在 ssh 或者一些网络应用的命令行工具时，也可以使用代理选项来使用代理服务器。

在很多网络请求场景中，都可以使用正向代理。



### 反向代理

对于 client，直接访问当目标服务器 server（proxy）。但目标服务器 server 可能会将请求转发到其它应用服务器 server（app）（即这个过程是 server 主动的过程）。

其对用户是透明的，如用户去访问 example.com，他并不知道该网站背后发生了什么事，一个 API 请求被转发到哪台服务器。

比如我们常用的 nginx，openresty 等软件。用于实现负载均衡和高可用。





### 代理软件的部署

**常用代理软件**

<https://github.com/elazarl/goproxy>

https://www.cnblogs.com/bluestorm/p/9032086.html



#### 出口代理

可以将代理固定在本地网络或者本地计算机的出口点，以便控制本地网络与大型因特网之间的流量。

比如在电脑的浏览器上配置代理规则。

**客户端代理**

- 手工配置浏览器

  只能为所有内容指定唯一的一个代理服务器

- 配置PAC代理

  提供一个 URI，指向一个用 JavaScript 语言编写的代理自动配置文件；客户端会取回这个 JavaScript 文件，并运行它以决定是否应该使用代理

  PAC 文件的后缀通常是 .pac，MIME 类型通常是 application/x-ns-proxy-autoconfig

  每个 PAC 文件都必须定义一个名为 FindProxyForURL(url,host) 的函数，用来计算访问 URI 时使用的适当的代理服务器

  ```javascript
  // 根据url的scheme不同使用不同的代理
  function FindProxyForURL(url, host) {
      if (url.substring(0, 5) == "http:") {
          return "PROXY http-proxy.mydomain.com:8080";
      } else if (url.substring(0, 4) == "ftp:") {
          return "PROXY ftp-proxy.mydomain.com:8080";
      } else {
          return "DIRECT";
      }
  }
  ```








#### 透明代理

网络基础设施可以通过若干种技术手段，在客户端不知道，或没有参与的情况下，拦截网络流量并将其导入代理。

这种拦截通常都依赖于监视 HTTP 流量的交换设备及路由设备，在客户端毫不知情的情况下，对其进行拦截，并将流量导入一个代理。

比如常用的软路由，在网关处设置代理等等。





### 代理相关的首部

#### VIA首部

Via 首部字段列出了与报文途经的每个中间节点（代理或网关）有关的信息。

报文每经过一个节点，都必须将这个中间节点添加到 Via 列表的末尾。

代理也可以用 Via 首部来检测网络中的路由循环。代理应该在发送一条请求之前，在 Via 首部插入一个与其自身有关的独特字符串，并在输入的请求中查找这个字符串，以检测网络中是否存在路由循环。

请求和响应报文都会经过代理进行传输，因此，请求和响应报文中都要有 Via首部

请求和响应通常都是通过同一条 TCP 连接传送的，所以响应报文会沿着与请求报文相同的路径回传

如果一条请求报文经过了代理 A、B 和 C，相应的响应报文就会通过代理 C、B、A 进行传输。因此，响应的 Via 首部基本上总是与请求的 Via 首部相反

```
# 报文流经了两个代理,第一个代理名为 proxy-62.irenes-isp.net ，它实现了 HTTP/1.1 协议.第二个代理名为 cache.joes-hardware.com
Via: 1.1 proxy-62.irenes-isp.net, 1.0 cache.joes-hardware.com
```



### 追踪报文



代理服务器可以在转发报文时对其进行修改。可以添加、修改或删除首部，也可以将主体部分转换成不同的格式。

代理变得越来越复杂，开发代理产品的厂商也越来越多，互操作性问题也开始逐渐显现。

通过 HTTP/1.1 的 TRACE 方法，用户可以跟踪经代理链传输的请求报文，观察报文经过了哪些代理，以及每个代理是如何对请求报文进行修改的。

当 TRACE 请求到达目的服务器时，整条请求报文都会被封装在一条 HTTP 响应的主体中回送给发送端。

```
# 请求报文
TRACE /index.html HTTP/1.1
Host: www.joes-hardware.com
Accept: text/html


# 响应头部
HTTP/1.1 200 OK
Content-Type: message/http
Content-Length: 269
Via: 1.1 cache.joes-hardware.com, 1.1 p1127.att.net, 1.1 proxy.irenes-isp.net

# 响应主体
TRACE /index.html HTTP/1.1
Host: www.joes-hardware.com
Accept: text/html
Via: 1.1 proxy.irenes-isp.net, 1.1 p1127.att.net, 1.1 cache.joes-hardware.com
X-Magic-CDN-Thingy: 134-AF-0003
Cookie: access-isp="Irene's ISP, California"
Client-ip: 209.134.49.32
```







### 代理认证



### 代理的互操作性

客户端、服务器和代理是由不同厂商构建的，实现的是不同版本的 HTTP 规范

它们支持的特性各不相同，也存在着不同的问题。



#### Allow首部

通过 HTTP OPTIONS 方法，客户端（或代理）可以发现 Web 服务器或者其上某个特定资源所支持的功能





## 缓存



当 Web 请求抵达缓存时，如果 "本地" 有 "已缓存" 的副本，就可以从本地存储设备而不是原始服务器中提取这个文档。（并不一定是本地计算机）

缓存是 Web 性能优化的一个很重要的优化手段。



缓存是一种保存资源副本并在下次请求时直接使用该副本的技术。当 web 缓存发现请求的资源已经被存储，它会拦截请求，返回该资源的拷贝，而不会去源服务器重新下载。

这样带来的好处有：**缓解服务器端压力，提升性能(获取资源的耗时更短了)。对于网站来说，缓存是达到高性能的重要组成部分。**

缓存需要合理配置，因为并不是所有资源都是永久不变的：重要的是对一个资源的缓存应截止到其下一次发生改变（即不能缓存过期的资源）。





### 缓存操作的目标



虽然 HTTP 缓存不是必须的，但重用缓存的资源通常是必要的。然而常见的 HTTP 缓存只能存储 [`GET`](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Methods/GET) 响应，对于其他类型的响应则无能为力。

缓存的关键主要包括 request method 和目标 URI（一般只有GET请求才会被缓存）。



 

Web 缓存的分类：服务器缓存(代理服务器缓存、CDN 缓存)，第三方缓存，浏览器缓存等。

缓存的种类有很多,其大致可归为两类：私有与共享缓存。共享缓存存储的响应能够被多个用户使用。私有缓存只能用于单独用户。

- 私有缓存（浏览器缓存）
- 共享缓存（CDN缓存，反向代理缓存，负载均衡器缓存，网关缓存）





### 缓存的新鲜度检测

理论上来讲，当一个资源被缓存存储后，该资源应该可以被永久存储在缓存中。

由于缓存只有有限的空间用于存储资源副本，所以缓存会定期地将一些副本删除（不可能无限缓存），这个过程叫做**缓存驱逐**。



另一方面，当服务器上面的资源进行了更新，那么缓存中的对应资源也应该被更新，由于HTTP是C/S模式的协议，服务器更新一个资源时，不可能直接通知客户端（CDN或者其他缓存）更新缓存内容。





缓存对缓存的副本进行再验证时，会向原始服务器发送一个小的再验证请求。如果内容没有变化，服务器会以一个小的 304 Not Modified 进行响应。

只要缓存知道副本仍然有效，就会再次将副本标识为暂时新鲜的，并将副本提供给客户端，这称为 **再验证命中** 。



**缓存命中和未命中**

当请求到达缓存时，会在缓存中检查资源是否存在，如果存在，则直接返回，称为**缓存命中**，如果不存在，则继续向下游服务器请求资源，称为**缓存未命中**。



HTML5 中引入了应用程序缓存，这意味着 web 应用可进行缓存，并可在没有因特网连接时进行访问。

- 离线浏览 - 用户可在应用离线时使用它们
- 速度 - 已缓存资源加载得更快
- 减少服务器负载 - 浏览器将只从服务器下载更新过或更改过的资源。

目前基本上所有的浏览器都支持 HTML5 缓存技术。不过这项技术已经从 WEB 标准中废除，基本上可以不用了解。




大部分缓存只有在客户端发起请求，并且副本旧得足以需要检测的时候，才会对副本进行再验证。





#### Cache-Control 首部（强缓存）

cache-control 首部是 HTTP1.1 引入的首部，请求头和响应头都支持这个属性。通过它提供的不同的值来定义缓存策略。

**请求时的缓存指令**包括: no-cache、no-store、max-age、 max-stale、min-fresh、only-if-cached等。
**响应消息中的指令**包括: public、private、no-cache、no- store、no-transform、must-revalidate、proxy-revalidate、max-age。



**没有缓存**

```html
Cache-Control: no-store
```

缓存中不得存储任何关于客户端请求和服务端响应的内容。每次由客户端发起的请求都会下载完整的响应内容。

（）





**缓存但重新验证**

```html
Cache-Control: no-cache
```

每次有请求发出时，缓存会将此请求发到服务器。（这个请求应该要带有与本地缓存验证的字段）

服务器端会验证请求中所描述的缓存是否过期，若未过期（注：实际就是返回304），则缓存才使用本地缓存副本。





**缓存过期**

```html
Cache-Control: max-age=31536000
```

Cache-Control:max-age  其中 max-age 指定了文档的最大使用期限。最大的合法生存时间为 484200 秒 。

针对应用中那些不会改变的文件，通常可以手动设置一定的时长以保证缓存有效，例如图片、css、js等静态资源。

Cache-Control: max-age=0 服务器可以请求缓存不缓存文档，让每次访问都请求到服务器上。

相对[Expires](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Expires)而言，max-age是距离请求发起的时间的秒数。即这个缓存有效时间是相对上一次访问的时间。







**私有和公共缓存**

```html
Cache-Control: private
Cache-Control: public
```

"public" 指令表示该响应可以被任何中间人（注：比如中间代理、CDN等）缓存。

若指定了"public"，则一些通常不被中间人缓存的页面（译者注：因为默认是private）（比如 带有HTTP验证信息（帐号密码）的页面 或 某些特定状态码的页面），将会被其缓存。

 "private" 则表示该响应是专用于某单个用户的，中间人不能缓存此响应，该响应只能应用于浏览器私有缓存中。









#### Expires 首部（强缓存机制）

- **值**：是一个GMT时间格式的绝对时间，`Expires` 的日期时间必须是格林威治时间（GMT），而不是本地时间。举例：`Expires: Fri, 30 Oct 1998 14:19:41`

- **作用**：告诉缓存器相关副本在多长时间内是新鲜的。过了这个时间，缓存器就会向源服务器发送请求，检查文档是否被修改。

- **兼容性**：几乎所有的缓存服务器都支持 `Expires`（过期时间）属性

  **规则**：基于客户最后查看副本的时间（最后访问时间）或者根据服务器上文档最后被修改的时间

  应用

  - 对于设置静态图片文件（例如导航栏和图片按钮）缓存特别有用；因为这些图片修改很少，你可以给它们设置一个特别长的过期时间，这会使你的网站对用户变得相应非常快。
  - 对于控制有规律改变的网页也很有用，例如：你每天早上6点更新新闻页，你可以设置副本的过期时间也是这个时间，这样缓存服务器就知道什么时候去取一个更新版本，而不必让用户去按浏览器的"刷新"按钮。
  - 过期时间头信息属性值只能是HTTP格式的日期时间，其他的都会被解析成当前时间"之前"，副本会过期

  局限性

  虽然过期时间属性非常有用，但是它还是有些局限， 

  - 首先：是牵扯到了日期，这样Web服务器的时间和缓存服务器的时间必须是同步的，如果有些不同步，要么是应该缓存的内容提前过期了，要么是过期结果没及时更新。
  - 如果你设置的过期时间是一个固定的时间，如果你返回内容的时候又没有连带更新下次过期的时间，那么之后所有访问请求都会被发送给源Web服务器，反而增加了负载和响应时间

  

#### If-Modified-Since 首部

If-Modified-Since 是一个请求头部，他用于指定内容修改时间。

将这个首部添加到 GET 请求中去，If-Modified-Since:Date 

- 如果自指定日期后，文档被修改了，通常 GET 就会成功执行，携带新首部的新文档会被返回给缓存，新首部中包含了新的过期时间。
- 如果自指定日期后，文档没被修改过，条件就为假，



#### If-None-Match 首部

If-None-Match 是一个请求头部，用于指定请求文档的版本号。

```
If-None-Match: W/"43cd67b71ec96ce713c66db2315e23cf"
```

当缓存向原始服务器请求时，就会检查这个版本号，响应报文中的 etag 就是资源的版本号。



#### Last-Modified 首部

If-Modified-Since 首部可以与 Last-Modified 服务器响应首部配合工作。

原始服务器会将最后的修改日期附加到所提供的文档上去。







### Memory Cache

https://help.aliyun.com/knowledge_detail/40077.html

我们用 chrome 访问很多页面时，如果刷新页面，F12 后可以看到，有些 HTTP 资源状态码是 200 。size 是 memory cache 。

Memory Cache 也就是内存中的缓存，主要包含的是当前中页面中已经抓取到的资源,例如页面上已经下载的样式、脚本、图片，字体等。

读取内存中的数据肯定比磁盘快，内存缓存虽然读取高效，可是缓存持续性很短，会随着进程的释放而释放。 **一旦我们关闭 Tab 页面，内存中的缓存也就被释放了**。



**那么既然内存缓存这么高效，我们是不是能让数据都存放在内存中呢？**

这是不可能的。计算机中的内存一定比硬盘容量小得多，操作系统需要精打细算内存的使用，所以能让我们使用的内存必然不多。

当我们访问过页面以后，再次刷新页面，可以发现很多数据都来自于内存缓存。

内存缓存中有一块重要的缓存资源是 preloader 相关指令（例如`<link rel="prefetch">`）下载的资源。总所周知 preloader 的相关指令已经是页面优化的常见手段之一，它可以一边解析 js/css 文件，一边网络请求下一个资源。

需要注意的事情是，**内存缓存在缓存资源时并不关心返回资源的HTTP缓存头Cache-Control是什么值，同时资源的匹配也并非仅仅是对 URL 做匹配，还可能会对 Content-Type，CORS 等其他特征做校验**。



![1607884065215](assets/1607884065215.png)





![1607884224020](assets/1607884224020.png)





![1607884512892](assets/1607884512892.png)



Like their names said:

"Memory Cache" stores and loads resources to and from Memory (RAM). So this is much faster but it is non-persistent. Content is available until you close the Browser.

"Disk Cache" is persistent. Cached resources are stored and loaded to and from disk.

Simple Test: Open Chrome Developper Tools / Network. Reload a page multiple times. The table column "Size" will tell you that some files are loaded "from memory cache". Now close the browser, open Developper Tools / Network again and load that page again. All cached files are loaded "from disk cache" now, because your memory cache is empty.





https://www.jianshu.com/p/54cc04190252

前端性能优化之资源预加载

https://blog.csdn.net/deaidai/article/details/86486496

Nginx 下关于缓存控制字段cache-control的配置说明

https://www.cnblogs.com/kevingrace/p/10459429.html



# HTTP2 

HTTP/2 可以让我们的应用更快、更简单、更稳定 - 这几词凑到一块是很罕见的！

HTTP/2 将很多以前我们在应用中针对 HTTP/1.1 想出来的“歪招儿”一笔勾销，把解决那些问题的方案内置在了传输层中。 

不仅如此，它还为我们进一步优化应用和提升性能提供了全新的机会！

HTTP/2 的主要目标是通过支持完整的请求与响应复用来减少延迟，通过有效压缩 HTTP 标头字段将协议开销降至最低，同时增加对请求优先级和服务器推送的支持。 

为达成这些目标，HTTP/2 还给我们带来了大量其他协议层面的辅助实现，例如新的流控制、错误处理和升级机制。

上述几种机制虽然不是全部，但却是最重要的，每一位网络开发者都应该理解并在自己的应用中加以利用。

**HTTP/2 没有改动 HTTP 的应用语义。 HTTP 方法、状态代码、URI 和标头字段等核心概念一如往常，这一点是非常重要的，可以理解为是向下兼容的。** 

不过，HTTP/2 修改了数据格式化（分帧）以及在客户端与服务器间传输的方式。这两点统帅全局，通过新的分帧层向我们的应用隐藏了所有复杂性。 

因此，所有现有的应用都可以不必修改而在新协议下运行。

HTTP 2.0 性能增强的核心，全在于新增的二进制分帧层，它定义了如何封装 HTTP 消息并在客户端与服务器之间传输。

![http2_binary_framing_layer](assets/http2_1.png.webp)



- **帧（Frame）**：HTTP/2 数据通信的最小单位。帧用来承载特定类型的数据，如 HTTP 首部、负荷；或者用来实现特定功能，例如打开、关闭流。每个帧都包含帧首部，其中会标识出当前帧所属的流；
- **消息（Message）**：指 HTTP/2 中逻辑上的 HTTP 消息。例如请求和响应等，消息由一个或多个帧组成；
- **流（Stream）**：存在于连接中的一个虚拟通道。流可以承载双向消息，每个流都有一个唯一的整数 ID；
- **连接（Connection）**：与 HTTP/1 相同，都是指对应的 TCP 连接；



- **在 HTTP/2 中，同域名下所有通信都在单个 TCP 连接上完成，这个连接可以承载任意数量的双向数据流。**

- **每个数据流都以消息的形式发送，而消息又由一个或多个帧组成。多个帧之间可以乱序发送，因为根据帧首部的流标识可以重新组装。**





### 请求与响应复用

在 HTTP/1.x 中，每一个请求和响应都要占用一个 TCP 连接。

尽管有 Keep-Alive 机制可以复用，但在每个 TCP  连接上同时只能有一个请求 / 响应，这意味着完成响应之前，这个连接不能用于其他请求。

如果客户端要想发起多个并行请求以提升性能，则必须使用多个 TCP 连接。 

这是 HTTP/1.x 交付模型的直接结果，该模型可以保证每个连接每次只交付一个响应（响应排队）。 

更糟糕的是，这种模型也会导致队首阻塞，从而造成底层 TCP 连接的效率低下。



大多数 HTTP 传输都是短暂且急促的，而 TCP 则针对长时间的批量数据传输进行了优化。

 通过重用相同的连接，HTTP/2 既可以更有效地利用每个 TCP 连接，也可以显著降低整体协议开销。 









HTTP/2 中新的二进制分帧层突破了这些限制，实现了完整的请求和响应复用：客户端和服务器可以将 HTTP 消息分解为互不依赖的帧，然后交错发送，最后再在另一端把它们重新组装起来。



> **HTTP/2 最大限度的兼容 HTTP/1.1 原有行为：**
>
> 1. **在应用层上修改，基于并充分挖掘 TCP 协议性能。**
> 2. **客户端向服务端发送 request 请求的模型没有变化。**
> 3. **scheme 没有发生变化，没有 http2://**
> 4. **使用 HTTP/1.X 的客户端和服务器可以无缝的通过代理方式转接到 HTTP/2 上。**
> 5. **不识别 HTTP/2 的代理服务器可以将请求降级到 HTTP/1.X。**



https://imququ.com/post/http2-and-wpo-2.html





#### 在 nodejs 中使用 HTTP2 

```javascript
// 示例代码app.js
const http2 = require('http2');
const fs = require('fs');

const server = http2.createSecureServer({
  key: fs.readFileSync('localhost-privkey.pem'),
  cert: fs.readFileSync('localhost-cert.pem')
});
server.on('error', (err) => console.error(err));

server.on('stream', (stream, headers) => {
  // stream is a Duplex
  stream.respond({
    'content-type': 'text/html; charset=utf-8',
    ':status': 200
  });
  stream.end('<h1>Hello World</h1>');
});

server.listen(8443,'0.0.0.0');
```





```shell
# 自签SSL证书



openssl req  -newkey  rsa:4096 \
	-x509 
	-sha256   \
    -nodes  \
    -sha256 \ 
    -subj  '/CN=localhost' \
  	-keyout localhost-privkey.pem  \
    -out localhost-cert.pem \ 

-newkey rsa:4096-创建新的证书请求和4096位RSA密钥。默认值为2048位。
-x509 -创建X.509证书。
-sha256 -使用265位SHA（安全哈希算法）。
-days 3650 -认证证书的天数。 3650是10年。您可以使用任何正整数。
-nodes -创建没有密码的密钥。
-out example.crt -指定将新创建的证书写入的文件名。您可以指定任何文件名。
-keyout example.key -指定要写入新创建的私钥的文件名。您可以指定任何文件名。

-subj 指定基本信息
CN=-国家/地区名称。 ISO的两个字母缩写。
ST= -州或省名。
L= -地区名称。您所在的城市的名称。
O= -您组织的全名。
OU= -组织单位。
CN= -完全限定的域名。


# 运行项目
node app.js


# 客户端访问，验证 HTTP2 协议
```



#### 服务器推送



HTTP 2.0 新增的一个强大的新功能，就是服务器可以对一个客户端请求发送多个响应。





# cookie和session

HTTP Cookie（也叫 Web Cookie 或浏览器 Cookie）是服务器发送到用户浏览器并保存在本地的一小块数据。

它会在浏览器下次向同一服务器再发起请求时被携带并发送到服务器上。

（试想，如果没有 cookie，如果你进入一个购物网站并且尚未登陆，添加商品到购物车后，然后刷新页面，购物车就被清空。那会是多么麻烦）

通常，它用于告知服务端两个请求是否来自同一浏览器，如保持用户的登录状态。

Cookie 使基于[无状态](https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview#HTTP_is_stateless_but_not_sessionless)的HTTP协议记录稳定的状态信息成为了可能。



Cookie 曾一度用于客户端数据的存储，因当时并没有其它合适的存储办法而作为唯一的存储手段，但现在随着现代浏览器开始支持各种各样的存储方式，Cookie 渐渐被淘汰。

由于服务器指定 Cookie 后，浏览器的每次请求都会携带 Cookie 数据，会带来额外的性能开销（尤其是在移动环境下）。

新的浏览器API已经允许开发者直接将数据存储到本地，如使用 [Web storage API](https://developer.mozilla.org/zh-CN/docs/Web/API/Web_Storage_API) （本地存储和会话存储）或 [IndexedDB](https://developer.mozilla.org/zh-CN/docs/Web/API/IndexedDB_API) 。





### cookie的过程

当服务器收到 HTTP 请求时，服务器可以在响应头里面添加一个 [`Set-Cookie`](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Set-Cookie) 头部。

浏览器收到响应后通常会保存下 Cookie，之后对该服务器每一次请求中都通过 [`Cookie`](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Cookie) 请求头部将 Cookie 信息发送给服务器。

另外，Cookie 的过期时间、域、路径、有效期、适用站点都可以根据需要来指定。

```shell
# 服务器返回的浏览器的响应头中添加这个头部，设置cookie
Set-Cookie: yummy_cookie=choco; tasty_cookie=strawberry



# 接下来浏览器对服务器的请求报文中都会携带这些cookie
GET /sample_page.html HTTP/1.1
Host: www.example.org
Cookie: yummy_cookie=choco; tasty_cookie=strawberry
```



### cookie的生命周期

Cookie 的生命周期可以通过两种方式定义：

- 会话期 Cookie 是最简单的 Cookie：浏览器关闭之后它会被自动删除，也就是说它仅在会话期内有效。会话期Cookie不需要指定过期时间（`Expires`）或者有效期（`Max-Age`）。需要注意的是，有些浏览器提供了会话恢复功能，这种情况下即使关闭了浏览器，会话期Cookie 也会被保留下来，就好像浏览器从来没有关闭一样，这会导致 Cookie 的生命周期无限期延长。

- 持久性 Cookie 的生命周期取决于过期时间（`Expires`）或有效期（`Max-Age`）指定的一段时间。

  例如：

  ```shell
  Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT;
  ```

> **提示：**当Cookie的过期时间被设定时，设定的日期和时间只与客户端相关，而不是服务端。



如果您的站点对用户进行身份验证，则每当用户进行身份验证时，它都应重新生成并重新发送会话 Cookie，甚至是已经存在的会话 Cookie。

此技术有助于防止[会话固定攻击（session fixation attacks）](https://wiki.developer.mozilla.org/en-US/docs/Web/Security/Types_of_attacks#Session_fixation)，在该攻击中第三方可以重用用户的会话。



### cookie的作用域

`Domain` 和 `Path` 标识定义了Cookie的作用域：即允许 Cookie 应该发送给哪些 URL。





```javascript
//nodejs的demo项目设置cookie
var http = require('http');
var server = http.createServer(function(request, response)
{
    
    response.setHeader('X-Foo', 'bar');
    response.setHeader('Set-Cookie', ['type=ninja', 'language=javascript']); 
    
    response.end("Hello fengzhao\n");
});

server.listen(3000);

```



# 认证



有数百万的人在用 Web 进行私人事务处理，访问私有的数据。通过 Web 可以很方便地访问这些信息，但仅仅是方便访问还是不够的。

我们要保证只有特定的人能看到我们的敏感信息并且能够执行我们的特权事务。**并不是所有的信息都能够公开发布的。**

服务器需要通过某种方式来了解用户身份。一旦服务器知道了用户身份，就可以判定用户可以访问的事务和资源了。

认证就意味着要证明你是谁。通常是通过提供用户名和密码来进行认证的。HTTP 为认证提供了一种原生工具。尽管我们可以在
HTTP 的认证形式和 cookie 基础之上 "运行自己的" 认证工具，但在很多情况下，HTTP 的原生认证功能就可以很好地满足要求。

**认证就是要给出一些身份证明。**当出示像护照或驾照那样有照片的身份证件时，就给出了一些证据，说明你就是你所声称的那个人。在自动取款机上输入 PIN 码，或在计算机系统的对话框中输入了密码时，也是在证明你就是你所声称的那个人。



### HTTP 基本认证

HTTP basic 认证原生提供了一种 质询 / 响应（challenge/response）框架，简化了对用户的认证过程。它的具体过程如下：

- 客户端请求资源（）
- 服务端返回 401 ，响应头中包含 www-Authorization 首部，质询用户认证。要求用户提供身份信息。
- 客户端提供账号密码，账号密码拼接后经过 base64 编码然后包含在 Authorization 头部提交给服务端。
- 服务端解码并验证，然后返回资源。







#### base64编码



我们发现字符串“Man”的Base64编码是“TWFu”，那么这是怎么转换过来的呢？

不急，我们一个一个字符来分析：

字符"M"对应的ASCII编码是77，二进制形式即 01001101；

字符“a”对应的ASCII编码是97，二进制表现形式为 01100001；

字符“n”对应的ASCII编码为110，二进制形式为：01101110。



这三个字符的二进制位组合在一起就变成了一个24位的字符串“010011010110000101101110”。



接下来，我们从左至右，每次抽取6位作为1组（因为6位一共有2^6=64种不同的组合），因此每一组的6位又代表一个数字（0~63），接下来，我们查看索引表，找到这个数字对应的字符，就是我们最后的结果，是不是很简单呢？

https://blog.csdn.net/doujinlong1/article/details/86579369

https://www.cnblogs.com/ranyonsue/p/8615824.html







### 摘要认证









# HTTP安全





中间人攻击

- 服务器认证（客户端知道它们是在与真正的而不是伪造的服务器通话）。 

- 客户端认证（服务器知道它们是在与真正的而不是伪造的客户端通话）。 
- 完整性（客户端和服务器的数据不会被修改）。
- 加密（客户端和服务器的对话是私密的，无需担心被窃听）。 
- 效率（一个运行的足够快的算法，以便低端的客户端和服务器使用）。 
- 





### 对称加密



### 非对称加密





### 数字签名



除了加 / 解密报文之外，还可以用加密系统对报文进行签名（sign），以说明是谁编写的报文，同时证明报文未被篡改过。

**这种技术被称为数字签名（digital signing）**



数字签名是附加在报文上的特殊加密校验码。使用数字签名有以下两个好处。

- 签名可以证明是作者编写了这条报文。只有作者才会有最机密的私有密钥， 因
  此，只有作者才能计算出这些校验和。校验和就像来自作者的个人“签名”一样。



数字签名通常是用非对称公开密钥技术产生的。因为只有所有者才知道其私有密钥，所以可以将作者的私有密钥当作一种“指纹”使用。





# WEB性能优化最佳实践



近几年来，WPO（Web Performance Optimization，Web 性能优化）产业从无到有，快速增长，充分说明用户越来越重视速度方面的用户体验。

而且，在我们这个节奏越来越快、联系越来越紧密的世界，追求速度不仅仅是一种心理上的需要，更是一种由现实事例驱动的用户需求。



- 网站越快，用户的黏性越高；
- 网站越快，用户忠诚度更高； 
- 网站越快，用户转化率越高。



在软件交互中，哪怕 100~200 ms 左右的延迟，我们中的大多数人就会感觉到“拖拉”；

如果超过了 300 ms 的门槛，那就会说“反应迟钝”；而要是延迟达到 1000 ms（1s）这个界限，很多用户就会在等待响应的时候分神，有人会想入非非，有人恨不得忙点别的什么事儿。





简言之，速度是关键。要提高速度，必须先了解与之相关的各种因素，以及根本性的限制。

本章主要介绍对所有网络通信都有决定性影响的两个方面：延迟和带宽



- 延迟：分组从信息源发送到目的地所需的时间
- 带宽：逻辑或物理通信路径最大的吞吐量



> 
>
> tracert 路由跟踪
>
> 
>
> http://einverne.github.io/post/2017/06/traceroute.html



> 为减少跨大西洋的延迟而铺设 Hibernia Express 专线
>
>
> 在金融市场上，很多常用交易算法首要的考虑因素就是延迟，因为几 ms 的差距可能导致数百万美元的收益或损失。
> 2011 年初，华为与 Hibernia Atlantic 开始合作铺设一条横跨大西洋，连接伦敦和纽约的近 5000 km 的海底光缆（Hibernia Express）。
>
> 铺设这条海底光缆的唯一目的，就是减少城市间的路由，（相对于使用其他横跨大西洋的线路）为交易商节省 5 ms 的延迟。
>
> 开通运营后，这条光缆将只由金融机构使用，耗资预计达 4 亿美元。
> 简单计算一下，不难得出节省 1 ms 的成本是 8000 万美元。延迟的代价由此可见一斑。











































> 今日头条面试题之一
>
> 
>
> 网页中的图片资源为什么分放在不同的域名下 ?  这个技术
>
> 比如知乎中的图片资源的URL通常是 https://pic1.zhimg.com/013926975_im.jpg ，与主站的域名不同。
>
>  
>
> 一、
>
> 浏览器针对同一个域名的并发请求是有限制的，Chrome是 6 个。即在Chrome中访问 zhihu.com 时，同时最多只能有6个TCP连接
>
> 浏览器对并发请求的数目限制是针对域名的，即针对同一域名（包括二级域名）在同一时间支持的并发请求数量的限制。
>
> 如果请求数目超出限制，则会阻塞。因此，网站中对一些静态资源，使用不同的一级域名，可以提升浏览器并行请求的数目，加速界面资源的获取速度。
>
> 浏览针对同一个域名的并发请求限制到底是什么意思？

