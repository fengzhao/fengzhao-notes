## 1.内存概述

我们通常所说的内存容量，比如常常说的某笔记本（某服务器内存容量）8GB，16GB，其实指的是物理内存。

**物理内存也称为主存，大多数计算机用的主存都是动态随机访问内存（DRAM）。**

操作系统资源调度功能之一，就是**内存分配管理调度**。所以只有内核才可以直接访问物理内存。那么业务进程要申请内存时，该怎么办呢？

Linux 内核给每个进程都提供了一个独立的虚拟地址空间，并且这个地址空间是连续的。这样进程就可以很方便地访问内存，更确切地说是访问虚拟内存。



**操作系统位数**

CPU内部所能够一次性**处理、传输、暂时存储**的信息量的最大长度定义为一个字长，32位或者64位结构的CPU指的就是字长的大小。**这里的32位，是指32bit**





在计算机中，CPU的地址总线数目决定了CPU的寻址范围，这种由地址总线对应的地址称作为物理地址。

比如最常见的32位和64位系统：

假如CPU有32根地址总线（一般情况下32位的CPU的地址总线是32位，也有部分32位的CPU地址总线是36位的，比如用做服务器的CPU）

那么提供的可寻址物理地址范围为2^32=4GB

（**在这里要注意一点，我们平常所说的32位CPU和64位CPU指的是CPU一次能够处理的数据宽度，即位宽，不是地址总线的数目**）。

所以我们装电脑，经常说32位操作系统，只能识别4GB内存，就是这个意思。

那么64位操作系统最大支持多少内存，可以算一算。这是一篇[参考链接](https://www.crucial.cn/articles/about-memory/how-much-memory-does-your-windows-support)

X86架构中，存在着3种地址概念：

- 逻辑地址：由 段寄存器+offset 组成，也是我们代码中地址存在的形式
- 线性地址（虚拟地址）：X86 分段模块（物理硬件）将 段寄存器+offset 组合起来后得到的地址
- 物理地址：CPU 访问物理内存所使用的地址

## 虚拟内存/内存映射

在计算机的世界里内存地址用来定义数据在内存中的存储位置的，内存地址也分为**虚拟地址**和**物理地址**。

比如现在的广东省深圳市在过去叫宝安县，河北省的石家庄过去叫常山，安徽省的合肥过去叫泸州。

不管是常山也好，石家庄也好，又或是合肥也好，泸州也罢，这些都是人为定义的名字而已，但是地方还是那个地方，它所在的地理位置是不变的。也就说虚拟地址可以人为的变来变去，但是物理地址永远是不变的。

虚拟地址也是人为设计的一个概念，类比我们现实世界中的收货地址，而物理地址则是数据在物理内存中的真实存储位置，类比现实世界中的城市，街道，小区的真实地理位置。



当程序运行起来之后就变成了进程，而这些业务数据结构的引用在进程的视角里全都都是虚拟内存地址，因为进程无论是在用户态还是在内核态能够看到的都是虚拟内存空间，物理内存空间被操作系统所屏蔽进程是看不到的。

进程通过虚拟内存地址访问这些数据结构的时候，虚拟内存地址会在内存管理子系统中被转换成物理内存地址，通过物理内存地址就可以访问到真正存储这些数据结构的物理内存了。

随后就可以对这块物理内存进行各种业务操作，从而完成业务逻辑。



### 为什么要使用虚拟内存

假设现在没有虚拟内存地址，我们在程序中对内存的操作全都都是使用物理内存地址。

在这种情况下，编写代码时就需要精确的知道每一个变量在内存中的具体位置，我们需要手动对物理内存进行布局，明确哪些数据存储在内存的哪些位置，除此之外我们还需要考虑为每个进程究竟要分配多少内存？内存紧张的时候该怎么办？如何避免进程与进程之间的地址冲突？等等一系列复杂且琐碎的细节。

现代操作系统中往往支持多个进程，需要处理多进程之间的协同问题，在多进程系统中直接使用物理内存地址操作内存所带来的上述问题就变得非常复杂了

```java
public static void main(String[] args) throws Exception {
        
        string i = args[0];
        ..........
  }
```

在程序代码相同的情况下，我们用这份代码同时启动三个 JVM 进程，我们暂时将进程依次命名为 a , b , c 。

这三个进程用到的代码是一样的，都是我们提前写好的，可以被多次运行。

由于我们是直接操作物理内存地址，假设变量 i 保存在 `0x354` 这个物理地址上。这三个进程运行起来之后，同时操作这个 0x354 物理地址，这样这个变量 i 的值不就混乱了吗？ 三个进程就会出现变量的地址冲突。



在直接操作物理内存的情况下，我们需要知道每一个变量的位置都被安排在了哪里，而且还要注意和多个进程同时运行的时候，不能共用同一个地址，否则就会造成地址冲突。

现实中一个程序会有很多的变量和函数，这样一来我们给它们都需要计算一个合理的位置，还不能与其他进程冲突，这就很复杂了。



> 程序局部性原理表现为：时间局部性和空间局部性。时间局部性是指如果程序中的某条指令一旦执行，则不久之后该指令可能再次被执行；如果某块数据被访问，则不久之后该数据可能再次被访问。空间局部性是指一旦程序访问了某个存储单元，则不久之后，其附近的存储单元也将被访问。

从程序局部性原理的描述中我们可以得出这样一个结论：**进程在运行之后，对于内存的访问不会一下子就要访问全部的内存，相反进程对于内存的访问会表现出明显的倾向性，更加倾向于访问最近访问过的数据以及热点数据附近的数据。**

根据这个结论我们就清楚了，无论一个进程实际可以占用的内存资源有多大，根据程序局部性原理，在某一段时间内，进程真正需要的物理内存其实是很少的一部分，我们只需要为每个进程分配很少的物理内存就可以保证进程的正常执行运转。

而虚拟内存的引入正是要解决上述的问题，虚拟内存引入之后，进程的视角就会变得非常开阔，每个进程都拥有自己独立的虚拟地址空间，进程与进程之间的虚拟内存地址空间是相互隔离，互不干扰的。每个进程都认为自己独占所有内存空间，自己想干什么就干什么。



从应用进程的视角，操作系统内核对物理内存地址进程了封装。

Linux 内核给每个进程都提供了一个独立的虚拟地址空间，并且这个地址空间是连续的。这样进程就可以很方便地访问内存，更确切地说是访问虚拟内存。

进程的视角就会变得非常开阔，每个进程都拥有自己独立的虚拟地址空间，进程与进程之间的虚拟内存地址空间是相互隔离，互不干扰的。

每个进程都认为自己独占所有内存空间，自己想干什么就干什么。

无论一个进程实际可以占用的内存资源有多大，根据程序局部性原理，在某一段时间内，进程真正需要的物理内存其实是很少的一部分，我们只需要为每个进程分配很少的物理内存就可以保证进程的正常执行运转。

虚拟地址空间的内部又被分为**内核空间**和**用户空间**两部分，不同字长（也就是单个 CPU 指令可以处理数据的最大长度，也就是CPU位数）的处理器，地址空间的范围也不同。



### Linux 进程虚拟内存空间



**在 32 位机器上，指针的寻址范围为 2^32，所能表达的虚拟内存空间为 4 GB。**所以在 32 位机器上进程的虚拟内存地址范围为：0x0000 0000 — 0xFFFF FFFF

- 其中**用户态虚拟内存空间为 3 GB**，虚拟内存地址范围为：0x0000 0000 - 0xC000 000

- 其中**内核态虚拟内存空间为 1 GB**，虚拟内存地址范围为：0xC000 000 - 0xFFFF FFFF



用户态虚拟内存空间中的代码段并不是从 0x0000 0000 地址开始的，而是从 0x0804 8000 地址开始。

0x0000 0000 到 0x0804 8000 这段虚拟内存地址是一段不可访问的保留区，因为在大多数操作系统中，数值比较小的地址通常被认为不是一个合法的地址，这块小地址是不允许访问的。



**在 64 位机器上，指针的寻址范围为 2^64，所能表达的虚拟内存空间为 16 EB 。**虚拟内存地址范围为：0x0000 0000 0000 0000 0000 - 0xFFFF FFFF FFFF FFFF 。

事实上在目前的 64 位系统下只使用了 48 位来描述虚拟内存空间，寻址范围为 2^48 ，所能表达的虚拟内存空间为 256TB

- 低 128 T 表示用户态虚拟内存空间，虚拟内存地址范围为：0x0000 0000 0000 0000 - 0x0000 7FFF FFFF F000
- 高 128 T 表示内核态虚拟内存空间，虚拟内存地址范围为：0xFFFF 8000 0000 0000 - 0xFFFF FFFF FFFF FFFF

这样一来就在用户态虚拟内存空间与内核态虚拟内存空间之间形成了一段 0x0000 7FFF FFFF F000 - 0xFFFF 8000 0000 0000 的地址空洞，我们把这个叫做 canonical address 空洞。







### 虚拟内存地址空间

一个进程运行起来是为了执行我们交代给进程的工作，执行这些工作的步骤我们通过程序代码事先编写好，然后编译成二进制文件存放在磁盘中，CPU 会执行二进制文件中的机器码来驱动进程的运行。

所以在进程运行之前，这些存放在二进制文件中的机器码需要被加载进内存中，而用于存放这些机器码的虚拟内存空间叫做**代码段**



在程序运行起来之后，总要操作变量吧，在程序代码中我们通常会定义大量的全局变量和静态变量，这些全局变量在程序编译之后也会存储在二进制文件中，在程序运行之前，这些全局变量也需要被加载进内存中供程序访问。

所以在虚拟内存空间中也需要一段区域来存储这些全局变量。数据段





### 进程如何使用虚拟内存？

对任何一个普通进程来讲，它都会涉及到5种不同的数据段。稍有编程知识的朋友都能想到这几个数据段中包含有“程序代码段”、“程序数据段”、“程序堆栈段”等。

- **代码段**：代码段是用来存放可执行文件的操作指令，也就是说是它是可执行程序在内存中的镜像。代码段需要防止在运行时被非法修改，所以只准许读取操作，而不允许写入（修改）操作。

  - 一个进程运行起来是为了执行我们交代给进程的工作，执行这些工作的步骤我们通过程序代码事先编写好，然后编译成二进制文件存放在磁盘中，CPU 会执行二进制文件中的机器码来驱动进程的运行。所以在进程运行之前，这些存放在二进制文件中的机器码需要被加载进内存中，而用于存放这些机器码的虚拟内存空间叫做代码段。

- **数据段**：数据段用来存放可执行文件中已初始化全局变量，换句话说就是存放程序静态分配的变量和全局变量。

  - 在程序代码中我们通常会定义大量的全局变量和静态变量，这些全局变量在程序编译之后也会存储在二进制文件中，在程序运行之前，这些全局变量也需要被加载进内存中供程序访问。
  - 那些在代码中被我们**指定了初始值的全局变量和静态变量**在虚拟内存空间中的存储区域我们叫做数据段。

- **BSS段**：BSS段包含了程序中**未初始化的全局变量和静态变量**，在内存中 bss段全部置零。

- **堆（heap）：程序在运行期间往往需要动态的申请内存，所以在虚拟内存空间中也需要一块区域来存放这些动态申请的内存，这块区域就叫做堆。注意这里的堆指的是 OS 堆并不是 JVM 中的堆**

  - 堆是用于存放进程运行中被**动态分配的内存段**，它的大小并不固定，可动态扩张或缩减。
  - 当进程调用malloc等函数分配内存时，新分配的内存就被动态添加到堆上（堆被扩张）；当利用free等函数释放内存时，被释放的内存从堆中被剔除（堆被缩减）

- **文件映射与匿名映射区**：程序在运行过程中还需要依赖动态链接库，这些动态链接库以 .so 文件的形式存放在磁盘中，比如 C 程序中的 glibc，里边对系统调用进行了封装。glibc 库里提供的用于动态申请堆内存的 malloc 函数就是对系统调用 sbrk 和 mmap 的封装。

  这些动态链接库也有自己的对应的代码段，数据段，BSS 段，也需要一起被加载进内存中。还有用于内存文件映射的系统调用 mmap，会将文件与内存进行映射，那么映射的这块内存（虚拟内存）也需要在虚拟地址空间中有一块区域存储。

  这些动态链接库中的代码段，数据段，BSS 段，以及通过 mmap 系统调用映射的共享内存区，在虚拟内存空间的存储区域叫做文件映射与匿名映射区。

- **栈**：栈是用户存放程序临时创建的局部变量，也就是说我们函数括弧“{}”中定义的变量（但不包括static声明的变量，static意味着在数据段中存放变量）。除此以外，在函数被调用时，其参数也会被压入发起调用的进程栈中，并且待到调用结束后，函数的返回值也会被存放回栈中。由于栈的先进先出特点，所以栈特别方便用来保存/恢复调用现场。从这个意义上讲，我们可以把堆栈看成一个寄存、交换临时数据的内存区。





以 Intel Core i7 处理器为例，64 位虚拟地址的格式为：全局页目录项（9位）+ 上层页目录项（9位）+ 中间页目录项（9位）+ 页内偏移（12位）。共 48 位组成的虚拟内存地址。

(从虚拟地址 0x00000000 到 0xBFFFFFFF)



**每个进程在用户态时，只能访问用户空间内存。只有进入内核态后，才可以访问内核空间内存。**

用户进程通常情况下只能访问用户空间的虚拟地址，不能访问内核空间虚拟地址。只有用户进程进行系统调用（代表用户进程在内核态执行）等时刻可以访问到内核空间。

虽然每个用户进程的地址空间都包含了内核空间，但这些内核空间，其实关联的都是相同的物理内存。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。

用户空间对应进程，所以每当进程切换，用户空间就会跟着变化；而内核空间是由内核负责映射，它并不会跟着进程改变，是固定的。

内核空间地址有自己对应的页表（init_mm.pgd），用户进程各自有不同的页表。

每个进程的用户空间都是完全独立、互不相干的。

不信的话，你可以把上面的程序同时运行10次（当然为了同时运行，让它们在返回前一同睡眠100秒吧），你会看到10个进程占用的线性地址一模一样。

既然每个进程都有一个这么大的地址空间，那么所有进程的虚拟内存加起来，自然要比实际的物理内存大得多。

所以，并不是所有的虚拟内存都会分配物理内存，只有那些实际使用的虚拟内存才分配物理内存，并且分配后的物理内存，是通过内存映射来管理的。

内存映射，其实就是将虚拟内存地址映射到物理内存地址。

为了完成内存映射，内核为每个进程都维护了一张页表（**页表实际上存储在 CPU 的内存管理单元 MMU 中**），记录虚拟地址与物理地址的映射关系：

- **对普通进程来说，它能看到的其实是内核提供的虚拟内存，这些虚拟内存还需要通过页表，由系统映射为物理内存。**
- **当进程通过 malloc() 申请内存后，内存并不会立即分配，而是在首次访问时，才通过缺页异常陷入内核中分配内存。通过伙伴系统或者 slab 分配器来分配。**
- **由于进程的虚拟地址空间比物理内存大很多，Linux 还提供了一系列的机制，应对内存不足的问题，比如缓存的回收、交换分区 Swap 以及 OOM 等。**

当进程访问的虚拟地址在页表中查不到时，系统会产生一个缺页异常，进入内核空间分配物理 内存、更新进程页表，最后再返回用户空间，恢复进程的运行。

**MMU 并不以字节为单位来管理内存，而是规定了一个内存映射的最小单位，也就是页，通常是 4 KB 大小。这样，每一次内存映射，都需要关联 4 KB 或者 4KB 整数倍的内存空间。**

<aside> 📢 这里就是常说的，操作系统分配内存的最小单位是页。

</aside>

页的大小只有 4 KB ，导致的另一个问题就是，整个页表会变得非常大。比方说，仅 32 位系统 就需要 100 多万个页表项（4GB/4KB），才可以实现整个地址空间的映射。

为了解决页表项过 多的问题，Linux 提供了两种机制，也就是**多级页表和大页（HugePage）**。

### 多级页表

多级页表就是把内存分成区块来管理，将原来的映射关系改成区块索引和区块内的偏移。

由于虚拟内存空间通常只用了很少一部分，那么，多级页表就只保存这些使用中的区块，这样就可以大大地减少页表的项数。

### 大页

大页，顾名思义，就是比普通页更大的内存块，常见的大小有 2MB 和 1GB。大页通常用在使用大量内存的进程上，比如 Oracle、DPDK 等。

因为内存分配是以页为单位进行分配的，默认4K，如果父进程此时操作的是一个bigkey，重新申请大块内存耗时会变长，可能会产生阻塞风险。

如果操作系统开启了内存大页机制（Huge Page，页面大小2M）。那么父进程申请内存时阻塞的概率将会大大提高，所以**在Redis机器上需要关闭Huge Page机制。**

```jsx
you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis
```



## **进程内存空间管理**

进程的虚拟内存空间管理，进程在内核中的描述符 task_struct 结构

```cpp
struct task_struct {
      // 进程id
	    pid_t				pid;
      // 用于标识线程所属的进程 pid
	    pid_t				tgid;
      // 进程打开的文件信息
      struct files_struct		*files;
      // 内存描述符表示进程的虚拟地址空间， 专门描述进程虚拟地址空间的内存描述符mm_struct 结构体中包含了前边几个小节中介绍的进程虚拟内存空间的全部信息。
      struct mm_struct		*mm;

     // .......... 省略 .......
}
```

每个进程都有唯一的 mm_struct 结构体，也就是前边提到的每个进程的虚拟地址空间都是独立，互不干扰的。



在Linux中主要提供了fork、vfork、clone三个进程创建方法。

1、**fork**

fork创建一个进程时，子进程只是完全复制父进程的资源，复制出来的子进程有自己的task_struct结构和pid，但却复制父进程其它所有的资源。

例如，要是父进程打开了五个文件，那么子进程也有五个打开的文件，而且这些文件的当前读写指针也停在相同的地方。所以这一步所做的是复制。

这样得到的子进程独立于父进程， 具有良好的并发性，但是二者之间的通讯需要通过专门的通讯机制，如：pipe，共享内存等机制，另外通过fork创建子进程，需要将上面描述的每种资源都复制一个副本。这样看来，fork是一个开销十分大的系统调用，这些开销并不是所有的情况下都是必须的，比如某进程fork出一个子进程后，其子进程仅仅是为了调用exec执行另一个可执行文件，那么在fork过程中对于虚存空间的复制将是一个多余的过程。



但由于现代Linux中是采取了**copy-on-write(COW写时复制)**技术，为了降低开销，fork最初并不会真的产生两个不同的拷贝，因为在那个时候，大量的数据其实完全是一样的。**写时复制**是在推迟真正的数据拷贝。

若后来确实发生了写入，那意味着parent和child的数据不一致了，于是产生复制动作，每个进程拿到属于自己的那一份，这样就可以降低系统调用的开销。所以有了写时复制后呢，vfork其实现意义就不大了。

fork()调用执行一次返回两个值，对于父进程，fork函数返回子程序的进程号，而对于子程序，fork函数则返回零，这就是一个函数返回两次的本质。

在fork之后，子进程和父进程都会继续执行fork调用之后的指令。子进程是父进程的副本。它将获得父进程的数据空间，堆和栈的副本，这些都是副本，父子进程并不共享这部分的内存。也就是说，子进程对父进程中的同名变量进行修改并不会影响其在父进程中的值。但是父子进程又共享一些东西，简单说来就是程序的正文段。正文段存放着由cpu执行的机器指令，通常是read-only的。

当我们调用 fork() 函数创建进程的时候，表示进程地址空间的 mm_struct 结构会随着进程描述符 task_struct 的创建而创建。

**子进程在新创建出来之后它的虚拟内存空间是和父进程的虚拟内存空间一模一样的，直接拷贝过来**。

通过 fork() 函数创建出的子进程，它的虚拟内存空间以及相关页表相当于父进程虚拟内存空间的一份拷贝，直接从父进程中拷贝到子进程中。

子进程共享了父进程的虚拟内存空间，这样子进程就变成了我们熟悉的线程。

**是否共享地址空间几乎是进程和线程之间的本质区别。Linux 内核并不区别对待它们，线程对于内核来说仅仅是一个共享特定资源的进程而已**。



如果你要查看某个进程占用的内存区域，可以使用命令 cat /proc/pid/maps 获得（pid是进程号），你可以发现很多类似于下面的数字信息。

子进程共享了父进程的虚拟内存空间，这样子进程就变成了我们熟悉的线程，**是否共享地址空间几乎是进程和线程之间的本质区别。**

**Linux 内核并不区别对待它们，线程对于内核来说仅仅是一个共享特定资源的进程而已**。

内核线程和用户态线程的区别就是内核线程没有相关的内存描述符 mm_struct ，内核线程对应的 task_struct 结构中的 mm 域指向 Null，所以内核线程之间调度是不涉及地址空间切换的。



### 内核如何划分用户态和内核态虚拟内存空间

进程的内存描述符 mm_struct 结构体中的 task_size 变量，task_size 定义了用户态地址空间与内核态地址空间之间的分界线。

```c
struct mm_struct {
    unsigned long task_size;    /* size of task vm space */
    unsigned long start_code, end_code, start_data, end_data;
    unsigned long start_brk, brk, start_stack;
    unsigned long arg_start, arg_end, env_start, env_end;
    unsigned long mmap_base;  /* base of mmap area */
    unsigned long total_vm;    /* Total pages mapped */
    unsigned long locked_vm;  /* Pages that have PG_mlocked set */
    unsigned long pinned_vm;  /* Refcount permanently increased */
    unsigned long data_vm;    /* VM_WRITE & ~VM_SHARED & ~VM_STACK */
    unsigned long exec_vm;    /* VM_EXEC & ~VM_WRITE & ~VM_STACK */
    unsigned long stack_vm;    /* VM_STACK */

       ...... 省略 ........
}
```



- start_code 和 end_code 定义代码段的起始和结束位置，程序编译后的二进制文件中的机器码被加载进内存之后就存放在这里。
- start_data 和 end_data 定义数据段的起始和结束位置，二进制文件中存放的全局变量和静态变量被加载进内存中就存放在这里。



### 虚拟内存空间分布

绝大部分高级语言都是用 C 语言编写的，包括 Java，申请内存必须经过 C 库，而 C 库通过预分配更大的空间作为内存池，来加快后续申请内存的速度。

这样，预分配的 6GB 的 C 库内存池就与 JVM 中预分配的 8G 内存池叠加在一起，造成了 Java 进程的内存占用超出了预期。

掌握内存池的特性，既可以避免写程序时内存占用过大，导致服务器性能下降或者进程OOM（Out Of Memory，内存溢出）被系统杀死，还可以加快内存分配的速度。

在系统空闲时申请内存花费不了多少时间，但是对于分布式环境下繁忙的多线程服务，获取内存的时间会上升几十倍。

另一方面，内存池是非常底层的技术，当我们理解它后，可以更换适合应用场景的内存池。

在多种编程语言共存的分布式系统中，内存池有很广泛的应用，优化内存池带来的任何微小的性能提升，都将被分布式集群巨大的主机规模放大，从而带来整体上非常可观的收益。

实际上，在你的业务代码与系统内核间，往往有两层内存池容易被忽略，尤其是其中的 C库内存池。

当代码申请内存时，首先会到达应用层内存池，如果应用层内存池有足够的可用内存，就会直接返回给业务代码，否则，它会向更底层的 C 库内存池申请内存。

比如，如果你在Apache、Nginx 等服务之上做模块开发，这些服务中就有独立的内存池。

当然，Java 中也有内存池，当通过启动参数 Xmx 指定 JVM 的堆内存为 8GB 时，就设定了 JVM 堆内存池的大小。

你可能听说过 Google 的 TCMalloc 和 FaceBook 的 JEMalloc，它们也是 C 库内存池。当 C 库内存池无法满足内存申请时，才会向操作系统内核申请分配内存。

Java 已经有了应用层内存池，为什么还会受到 C 库内存池的影响呢？

这是因为，除了 JVM 负责管理的堆内存外，Java 还拥有一些堆外内存，由于它不使用 JVM 的垃圾回收机制，所以更稳定、持久，处理 IO 的速度也更快。

这些堆外内存就会由 C 库内存池负责分配，这是 Java 受到 C 库内存池影响的原因。

其实不只是 Java，几乎所有程序都在使用 C 库内存池分配出的内存。C 库内存池影响着系统下依赖它的所有进程。

### C库内存池

我们就以 Linux 系统的默认 C 库内存池 Ptmalloc2 来具体分析，看看它到底对性能发挥着怎样的作用。

C 库内存池工作时，会预分配比你申请的字节数更大的空间作为内存池。

比如说，当主进程下申请 1 字节的内存时，Ptmalloc2 会预分配 132K 字节的内存（Ptmalloc2 中叫 MainArena），应用代码再申请内存时，会从这已经申请到的 132KB 中继续分配。

## 2.如何阅读free的输出？

**free**命令显示了Linux系统中物理内存、交换分区的使用统计信息。

使用free命令查看内存信息，最重要的是理解**当前系统的可用内存**并不是直接看 **free** 字段就可以看出来的，应该参考的是：`可用内存 = free + buffers + cached`

在 `Linux 2.4` 的内存管理中，`buffer` 指 Linux 内存的：`Buffer cache`。`cache` 指 Linux 内存中的：`Page cache`。一般呢，是这么解释两者的。

- A buffer is someting that has yet to be ‘written’ to disk.
- A cache is someting that has been ‘read’ from the disk and stored for later use.

翻译过来就是说：

1. buffer (buff) 是用来缓存尚未 “写入” 磁盘的内容。 `buffer` 被用来当成对 io 设备写的缓存。
2. cache 是用来缓存从磁盘 “读取” 出来的东西。 `cache` 被用来当作对 io 设备的读缓存。

这里的 io 设备，主要指的是块设备文件和文件系统上的普通文件。

**在 `Linux 2.6` 以后，它们的意义不一样了。**

- Memory used by kernel buffers (Buffers in /proc/meminfo)
- cache  Memory used by the page cache and slabs (Cached and SReclaimable in /proc/meminfo)
- Buffers 是内核缓冲区用到的内存，对应的是 /proc/meminfo 中的 Buffers 值。
- Cache 是内核页缓存和 Slab 用到的内存，对应的是 /proc/meminfo 中的 Cached 与 SReclaimable 之和。

Buffers %lu  Relatively temporary storage for raw disk blocks that shouldn't get tremendously large (20MB or so).

Cached %lu   In-memory cache for files read from the disk (the page cache).  Doesn't include SwapCached.

SwapCached %lu Memory  that  once was swapped out, is swapped back in but still also is in the swap file.

(If memory pressure is high, these pages don't need to be swapped out again because they are already in the swap file.  This saves I/O.)

-Buffers 是对原始磁盘块的临时存储，也就是用来缓存磁盘的数据，通常不会特别大（20MB 左右）。这样，内核就可以把分散的写集中起来，统一优化磁盘的写入，比如可以把多次小的写合并成单次大的写等等。

在 `Linux 2.6` 之后 Linux 将他们统一合并到了 `Page cache` 作为文件层的缓存。而 `buffer` 则被用作 `block` 层的缓存。

`block` 层的缓存是什么意思呢，你可以认为一个 `buffer` 是一个 `physical disk block` 在内存的代表，用来将内存中的 `pages` 映射为 `disk blocks`，这部分被使用的内存被叫做 `buffer`。

> buffer 里面的 pages，指的是 Page cache 中的 pages，所以，buffer 也可以被认为 Page cache 的一部分。

**或者简单来说，`buffer` 负责裸设备相关的缓存，`cache` 负责文件系统的缓存。**

## **Buffer**

在当前的系统实现里，`buffer` 主要是设计用来在系统对块设备进行读写时作为缓存来使用。这意味着对块的操作会使用 `buffer` 进行缓存，比如我们在格式化文件系统的时候。

但是一般情况下两个缓存系统是一起配合使用的，比如当我们对一个文件进行写操作的时候，`cache` 的内容会被改变，而 `buffer` 则用来将 `cache` 的 `page` 标记为不同的缓冲区，并记录是哪一个缓冲区被修改了。

这样，内核在后续执行脏数据的回写（`writeback`）时，就不用将整个 `page` 写回，而只需要写回修改的部分即可。

```bash
root@fengzhao-ubuntu ~# free -h
               total        used        free      shared   buff/cache   available
Mem:           7.6Gi       870Mi       5.6Gi        17Mi       1.2Gi       6.5Gi
Swap:          2.0Gi          0B       2.0Gi
root@fengzhao-ubuntu ~#
```

| 标题      | 说明                                                   |
| --------- | ------------------------------------------------------ |
| total     | 物理内存总量                                           |
| used      | 已使用内存总量，包含应用使用量+buffer+cached           |
| free      | 空闲内存总量                                           |
| shared    | 共享内存总量                                           |
| buffers   | 块设备所占用的缓存                                     |
| cached    | 普通文件数据所占用的缓存                               |
| available | 当前可用内存总量（可用于分配给应用的，不包含虚拟内存） |

通过 man free 查看手册，可以看到如下描述（可以看出这些数值都来自 /proc/meminfo）

```markdown
total          Total installed memory (MemTotal and SwapTotal in /proc/meminfo)
used           Used memory (calculated as total - free - buffers - cache)
free           Unused memory (MemFree and SwapFree in /proc/meminfo)
shared         Memory used (mostly) by tmpfs (Shmem in /proc/meminfo)
buffers        Memory used by kernel buffers (Buffers in /proc/meminfo)
cache          Memory used by the page cache and slabs (Cached and SReclaimable in /proc/meminfo)
buff/cache     Sum of buffers and cache
```

继续 man proc 查看proc文件系统的帮助手册：

```markdown
MemTotal       Total usable RAM (i.e., physical RAM minus a few reserved bits and the kernel binary code).
MemFree        The sum of LowFree+HighFree.
MemAvailable   An estimate of how much memory is available for starting new applications, without swapping.      (since Linux 3.14) 
Buffers        Relatively temporary storage for raw disk blocks that shouldn't get tremendously large (20 MB or so).
Cached         In-memory cache for files read from the disk (the page cache).  Doesn't include SwapCached.
SwapCached     Memory that once was swapped out, is swapped back in but still also is in the swap file.  (If memory pressure is high, these pages don't need to be swapped out again because they are already in the swap file.  This saves I/O.)
Active         Memory that has been used more recently and usually not reclaimed unless absolutely necessary.
Inactive       Memory which has been less recently used.  It is more eligible to be reclaimed for other purposes.
Active(anon)   [To be documented.] (since Linux 2.6.28)
Inactive(anon)  [To be documented.]  (since Linux 2.6.28)
Active(file)    [To be documented.]  (since Linux 2.6.28)
Inactive(file) [To be documented.] (since Linux 2.6.28)
                    

              Unevictable %lu (since Linux 2.6.28)
                     (From Linux 2.6.28 to 2.6.30, CONFIG_UNEVICTABLE_LRU was required.)  [To be documented.]

              Mlocked %lu (since Linux 2.6.28)
                     (From Linux 2.6.28 to 2.6.30, CONFIG_UNEVICTABLE_LRU was required.)  [To be documented.]
```